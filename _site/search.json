[
  {
    "objectID": "Take-Home_Ex/Take-Home_Ex1/Take-Home_Ex1.html",
    "href": "Take-Home_Ex/Take-Home_Ex1/Take-Home_Ex1.html",
    "title": "Take Home Exercise 1 - Geospatial Analytics for Public Good",
    "section": "",
    "text": "In the era of digital urbanization, city-wide infrastructures, encompassing transportation modes like buses, taxis, and mass transit, have undergone significant digitization. This transformation has yielded extensive datasets that serve as a fundamental framework for monitoring movement patterns across both space and time. This shift is particularly evident with the widespread adoption of pervasive computing technologies, including GPS and RFID, notably integrated into vehicles. For instance, the utilization of smart cards and GPS devices on public buses enables the collection of comprehensive data on routes and ridership. Within these vast datasets lie inherent structures and patterns that offer valuable insights into the characteristics of measured phenomena, providing a deeper understanding of human movement and behaviors within urban environments. The identification, analysis, and comparison of these patterns present opportunities for enhanced urban management, offering valuable information for both public and private urban transport service providers. Despite these possibilities, current practices often restrict the use of massive locational data to basic tracking and mapping through Geographic Information System (GIS) applications. This limitation arises from the insufficient capabilities of conventional GIS in effectively analyzing and modeling spatial and spatio-temporal data."
  },
  {
    "objectID": "Take-Home_Ex/Take-Home_Ex1/Take-Home_Ex1.html#overview",
    "href": "Take-Home_Ex/Take-Home_Ex1/Take-Home_Ex1.html#overview",
    "title": "Take Home Exercise 1 - Geospatial Analytics for Public Good",
    "section": "",
    "text": "In the era of digital urbanization, city-wide infrastructures, encompassing transportation modes like buses, taxis, and mass transit, have undergone significant digitization. This transformation has yielded extensive datasets that serve as a fundamental framework for monitoring movement patterns across both space and time. This shift is particularly evident with the widespread adoption of pervasive computing technologies, including GPS and RFID, notably integrated into vehicles. For instance, the utilization of smart cards and GPS devices on public buses enables the collection of comprehensive data on routes and ridership. Within these vast datasets lie inherent structures and patterns that offer valuable insights into the characteristics of measured phenomena, providing a deeper understanding of human movement and behaviors within urban environments. The identification, analysis, and comparison of these patterns present opportunities for enhanced urban management, offering valuable information for both public and private urban transport service providers. Despite these possibilities, current practices often restrict the use of massive locational data to basic tracking and mapping through Geographic Information System (GIS) applications. This limitation arises from the insufficient capabilities of conventional GIS in effectively analyzing and modeling spatial and spatio-temporal data."
  },
  {
    "objectID": "Take-Home_Ex/Take-Home_Ex1/Take-Home_Ex1.html#objective",
    "href": "Take-Home_Ex/Take-Home_Ex1/Take-Home_Ex1.html#objective",
    "title": "Take Home Exercise 1 - Geospatial Analytics for Public Good",
    "section": "Objective",
    "text": "Objective\nIn this study, we first perform\n\nExploratory Spatial Data Analysis (ESDA) to provide us an understanding of the movement patterns on a high level before proceeding to with either\nLocal Indicators of Spatial Association (LISA)\n\nto undercover the spatial and spatio-temporal mobility patterns of public bus passengers in Singapore in detail."
  },
  {
    "objectID": "Take-Home_Ex/Take-Home_Ex1/Take-Home_Ex1.html#task",
    "href": "Take-Home_Ex/Take-Home_Ex1/Take-Home_Ex1.html#task",
    "title": "Take Home Exercise 1 - Geospatial Analytics for Public Good",
    "section": "Task",
    "text": "Task\nThe specific tasks of this take-home exercise are as follows:\n\nGeovisualisation and Analysis\n\nWith reference to the time intervals provided in the table below, compute the passenger trips generated by origin at the hexagon level,\n\n\n\nPeak hour period\nBus tap on time\n\n\n\n\nWeekday morning peak\n6am to 9am\n\n\nWeekday afternoon peak\n5pm to 8pm\n\n\nWeekend/holiday morning peak\n11am to 2pm\n\n\nWeekend/holiday evening peak\n4pm to 7pm\n\n\n\nDisplay the geographical distribution of the passenger trips by using appropriate geovisualisation methods,\nDescribe the spatial patterns revealed by the geovisualisation (not more than 200 words per visual).\n\n\n\nLocal Indicators of Spatial Association (LISA) Analysis\n\nCompute LISA of the passengers trips generate by origin at hexagon level.\nDisplay the LISA maps of the passengers trips generate by origin at hexagon level. The maps should only display the significant (i.e.Â p-value &lt; 0.05)\nWith reference to the analysis results, draw statistical conclusions (not more than 200 words per visual)."
  },
  {
    "objectID": "Take-Home_Ex/Take-Home_Ex1/Take-Home_Ex1.html#objective-1",
    "href": "Take-Home_Ex/Take-Home_Ex1/Take-Home_Ex1.html#objective-1",
    "title": "Take Home Exercise 1 - Geospatial Analytics for Public Good",
    "section": "Objective",
    "text": "Objective"
  },
  {
    "objectID": "Take-Home_Ex/Take-Home_Ex1/Take-Home_Ex1.html#the-data",
    "href": "Take-Home_Ex/Take-Home_Ex1/Take-Home_Ex1.html#the-data",
    "title": "Take Home Exercise 1 - Geospatial Analytics for Public Good",
    "section": "The Data",
    "text": "The Data\n\nAspatial data\nFor the purpose of this take-home exercise, Passenger Volume by Origin Destination Bus Stops downloaded from LTA DataMall will be used.\n\n\nGeospatial data\nTwo geospatial data will be used in this study, they are:\n\nBus Stop Location from LTA DataMall. It provides information about all the bus stops currently being serviced by buses, including the bus stop code (identifier) and location coordinates.\nMaster Plan 2019 Planning Sub-zone (No Sea) GIS data set of URA from data.gov.sg"
  },
  {
    "objectID": "Take-Home_Ex/Take-Home_Ex1/Take-Home_Ex1.html#install-r-package",
    "href": "Take-Home_Ex/Take-Home_Ex1/Take-Home_Ex1.html#install-r-package",
    "title": "Take Home Exercise 1 - Geospatial Analytics for Public Good",
    "section": "Install R Package",
    "text": "Install R Package\n\npacman::p_load(sf, sfdep, magrittr, tidyverse, tmap, knitr, RColorBrewer, viridis)"
  },
  {
    "objectID": "Take-Home_Ex/Take-Home_Ex1/Take-Home_Ex1.html#importing-data",
    "href": "Take-Home_Ex/Take-Home_Ex1/Take-Home_Ex1.html#importing-data",
    "title": "Take Home Exercise 1 - Geospatial Analytics for Public Good",
    "section": "1. Importing Data",
    "text": "1. Importing Data\nWe will import the data as a first step before proceeding with data cleaning, data wrangling and data exploration for the following:\n\nPassengerVolume, a csv file,\nBusStop, a point feature layer ESRI shapefile format\n\n\nPassenger VolumeBus Stop Location\n\n\nPassengerVolume is an aspatial data, we can import the data simply by using the read_csv function from tidyverse package and output it as a tibble dataframe called odbus\n\nodbus &lt;- read_csv(\"data/aspatial/origin_destination_bus_202310.csv\")\n\n\n\nBus Stop is a geospatial data in .shp file. We save it as a sf data frame called busstop using the st_read function of the sf package. The data is then geo-referenced to coordinates from the Singapore SVY21 coordinate system (EPSG: 3414)\n\nbusstop &lt;- st_read(dsn = \"data/geospatial\", \n                   layer = \"BusStop\") %&gt;%\n  st_transform(crs=3414)\n\nReading layer `BusStop' from data source \n  `C:\\weipengten\\ISSS624\\Take-Home_Ex\\Take-Home_Ex1\\data\\geospatial' \n  using driver `ESRI Shapefile'\nSimple feature collection with 5161 features and 3 fields\nGeometry type: POINT\nDimension:     XY\nBounding box:  xmin: 3970.122 ymin: 26482.1 xmax: 48284.56 ymax: 52983.82\nProjected CRS: SVY21"
  },
  {
    "objectID": "Take-Home_Ex/Take-Home_Ex1/Take-Home_Ex1.html#data-wrangling",
    "href": "Take-Home_Ex/Take-Home_Ex1/Take-Home_Ex1.html#data-wrangling",
    "title": "Take Home Exercise 1 - Geospatial Analytics for Public Good",
    "section": "2. Data Wrangling",
    "text": "2. Data Wrangling\n\nPassenger VolumeBus Stop LocationHexagonal DatasetCombining the Datasets\n\n\n\nData Exploration\n\nglimpse(odbus)\n\nAs we intend to utilize Bus-stop codes as our unique identifiers when joining with our other datasets, it is not advisable to have it remain as a chr datatype. In fact, we should change it to a factor datatype.\n\nodbus$ORIGIN_PT_CODE &lt;- as.factor(odbus$ORIGIN_PT_CODE)\nodbus$DESTINATION_PT_CODE &lt;- as.factor(odbus$DESTINATION_PT_CODE) \n\n\n\nChecking for Duplicates\nThere is no duplicates\n\nduplicate &lt;- odbus %&gt;%\n  group_by_all() %&gt;%\n  filter(n()&gt;1) %&gt;%\n  ungroup()\nduplicate\n\n\n\nChecking for Missing Data\nThere is no missing data\n\nsummary(odbus)\n\n\n\nClassifying Peak Hours\nWith reference to the time intervals provided in the requirements, we computed the passenger trips generated by origin. The passenger trips by origin are saved in 4 dataframes based on their respective classifications namely:\n\nweekday_morning_peak\nweekday_afternoon_peak\nweekend_morning_peak\nweekend_evening_peak\n\n\n\nShow the code\nweekday_morning_peak &lt;- odbus %&gt;%\n  filter(DAY_TYPE == \"WEEKDAY\") %&gt;%\n  filter(TIME_PER_HOUR &gt;= 6 &\n           TIME_PER_HOUR &lt;= 9) %&gt;%\n  group_by(ORIGIN_PT_CODE) %&gt;%\n  summarise(TRIPS = sum(TOTAL_TRIPS))\n\nweekday_afternoon_peak &lt;- odbus %&gt;%\n  filter(DAY_TYPE == \"WEEKDAY\") %&gt;%\n  filter(TIME_PER_HOUR &gt;= 17 &\n           TIME_PER_HOUR &lt;= 20) %&gt;%\n  group_by(ORIGIN_PT_CODE) %&gt;%\n  summarise(TRIPS = sum(TOTAL_TRIPS))\n\nweekend_morning_peak &lt;- odbus %&gt;%\n  filter(DAY_TYPE == \"WEEKENDS/HOLIDAY\") %&gt;%\n  filter(TIME_PER_HOUR &gt;= 11 &\n           TIME_PER_HOUR &lt;= 14) %&gt;%\n  group_by(ORIGIN_PT_CODE) %&gt;%\n  summarise(TRIPS = sum(TOTAL_TRIPS))\n\nweekend_evening_peak &lt;- odbus %&gt;%\n  filter(DAY_TYPE == \"WEEKENDS/HOLIDAY\") %&gt;%\n  filter(TIME_PER_HOUR &gt;= 16 &\n           TIME_PER_HOUR &lt;= 19) %&gt;%\n  group_by(ORIGIN_PT_CODE) %&gt;%\n  summarise(TRIPS = sum(TOTAL_TRIPS))\n\n\nwrite_rds(weekday_morning_peak, \"data/rds/weekday_morning_peak.rds\")\nweekday_morning_peak &lt;- read_rds(\"data/rds/weekday_morning_peak.rds\")\n\nwrite_rds(weekday_afternoon_peak, \"data/rds/weekday_afternoon_peak.rds\")\nweekday_afternoon_peak &lt;- read_rds(\"data/rds/weekday_afternoon_peak.rds\")\n\nwrite_rds(weekend_morning_peak, \"data/rds/weekend_morning_peak.rds\")\nweekend_morning_peak &lt;- read_rds(\"data/rds/weekend_morning_peak.rds\")\n\nwrite_rds(weekend_evening_peak, \"data/rds/weekend_evening_peak.rds\")\nweekend_evening_peak &lt;- read_rds(\"data/rds/weekend_evening_peak.rds\")\n\n\nIn the code above, we have did a summation of Origin trips , grouped by the origin bus stop number for the 4 classifications through filtering for weekdays from weekends and by the stated time bins.\nWe save our processed data into .rds data format files using theÂ write_rds()Â ofÂ readrÂ package. The output file is saved inÂ rdsÂ sub-folder. We do this to reduce the loading time and more importantly, we can avoid uploading the large raw files onto GitHub.\n\n\n\n\nData is Clean!\nNot surprisingly, there are also no duplicates and missing data for Bus stop Data. The data is clean.\n\nduplicate &lt;- busstop %&gt;%\n  group_by_all() %&gt;%\n  filter(n()&gt;1) %&gt;%\n  ungroup()\nduplicate\n\nduplicate bus stops found, removing duplicates directlyâ¦\n\n\nShow the code\nduplicates &lt;- busstop[duplicated(busstop$BUS_STOP_N), ]\n\n# Check if there are any duplicates\nif (nrow(duplicates) &gt; 0) {\n  cat(\"Duplicate values found in the BUS_STOP_N column.\\n\")\n  print(duplicates)\n\n  # Remove duplicates from the original dataframe\n  busstop &lt;- busstop[!duplicated(busstop$BUS_STOP_N), ]\n  \n  cat(\"Duplicates removed from the BUS_STOP_N column.\\n\")\n} else {\n  cat(\"No duplicate values found in the BUS_STOP_N column.\\n\")\n}\n\n\nDuplicate values found in the BUS_STOP_N column.\nSimple feature collection with 16 features and 3 fields\nGeometry type: POINT\nDimension:     XY\nBounding box:  xmin: 13488.02 ymin: 32604.36 xmax: 44055.75 ymax: 47934\nProjected CRS: SVY21 / Singapore TM\nFirst 10 features:\n     BUS_STOP_N BUS_ROOF_N            LOC_DESC                  geometry\n338       58031        UNK     OPP CANBERRA DR POINT (27111.07 47517.77)\n2035      82221        B01              Blk 3A POINT (35308.74 33335.17)\n2038      97079        B14 OPP ST. JOHN'S CRES  POINT (44055.75 38908.5)\n2092      22501        B02            BLK 662A POINT (13488.02 35537.88)\n2237      62251        B03        BEF BLK 471B POINT (35500.36 39943.34)\n3158      53041        B07    Upp Thomson Road POINT (27956.34 37379.29)\n3261      77329        B03   Pasir Ris Central POINT (40728.15 39438.15)\n3265      96319        NIL     YUSEN LOGISTICS POINT (42187.23 34995.78)\n3303      52059        B09             BLK 219 POINT (30565.45 36133.15)\n3411      43709        B06             BLK 644 POINT (18952.02 36751.83)\nDuplicates removed from the BUS_STOP_N column.\n\n\nChecked duplicates removed successfully\n\n\nShow the code\nduplicates &lt;- busstop[duplicated(busstop$BUS_STOP_N), ]\n\n# Check if there are any duplicates\nif (nrow(duplicates) &gt; 0) {\n  cat(\"Duplicate values found in the BUS_STOP_N column.\\n\")\n  print(duplicates)\n} else {\n  cat(\"No duplicate values found in the BUS_STOP_N column.\\n\")\n}\n\n\nNo duplicate values found in the BUS_STOP_N column.\n\n\n\nsummary(busstop)\n\n\n\n\n\nCreate Hexagon Dataset from busstop\nNext we proceed to fulfill our requirement of preparing a hexagon dataset with specified cell dimensions of 250 by 250 units called hexagon using the st_make_grid function from the sf package.\nWe convert it into a sf dataframe called hexagon_sf using the st_sf function of sf package.\nThe code also adds a new variable/column called âgrid_idâ to the sf object. The âgrid_idâ values are assigned incrementally, starting from 1 and corresponding to the order of the hexagons in the grid. This step essentially assigns a unique identifier to each hexagon in the grid, facilitating further spatial analysis or mapping.\n\n\nShow the code\nhexagon = st_make_grid(busstop, c(250, 250), what = \"polygons\", square = FALSE)\n\n# To sf and add grid ID\nhexagon_sf = st_sf(hexagon) %&gt;%\n  # add grid ID\n  mutate(grid_id = 1:length(lengths(hexagon)))\n\n\n\n\nShow the code\nduplicates &lt;- hexagon_sf[duplicated(hexagon_sf$grid_id), ]\n\n# Check if there are any duplicates\nif (nrow(duplicates) &gt; 0) {\n  cat(\"Duplicate values found in the grid_id column.\\n\")\n  print(duplicates)\n} else {\n  cat(\"No duplicate values found in the grid_id column.\\n\")\n}\n\n\nNo duplicate values found in the grid_id column.\n\n\n\n\nExamine The Grid\nA brief overplot shows that there are 22134 grids in total and 19003 are without bus stops. We have a max of 5 bus stops per ORIGIN_GRID.\n\n\nShow the code\nhexagon_sf$n_colli = lengths(st_intersects(hexagon_sf, busstop))\ncount_all_grid_ids &lt;- n_distinct(hexagon_sf$grid_id)\ncount_zero_bus_stops &lt;- hexagon_sf %&gt;%\n  filter(n_colli == 0) %&gt;%\n  summarize(count = n_distinct(grid_id)) %&gt;%\n  pull(count)\nprint(count_all_grid_ids)\n\n\n[1] 22134\n\n\nShow the code\nprint(count_zero_bus_stops)\n\n\n[1] 19007\n\n\nShow the code\nsummary(hexagon_sf$n_colli)\n\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n 0.0000  0.0000  0.0000  0.2325  0.0000  5.0000 \n\n\n\n\nImportant step to ensure this dataset will be useful for us\nFilter for only hexagon data with non-zero counts of bus stops\n\nhexagon_sf = filter(hexagon_sf, n_colli &gt; 0)\nwrite_rds(hexagon_sf, \"data/rds/hexagon_sf.rds\")\nhexagon_sf &lt;- read_rds(\"data/rds/hexagon_sf.rds\")\n\n\n\nVIsualising the dataset\nWe can also do a visualisation to analyze the distribution of busstops. We specify break points at 0,1,2,3,4 and 5\nFrom the map below, it is obvious that most hexagons have 1 or 2 bus stops in their grid with some having 4 or 5 bus stops. There is approximately one âclusterâ that are close to each other and having 4 or 5 bus stops in each region in North, East, South, West.\n\n\nShow the code\ntmap_mode(\"plot\")\n\nmap_busstopcounts = tm_shape(hexagon_sf) +\n  tm_fill(\n    col = \"n_colli\",\n    palette = c(\"grey\",rev(viridis(5))),\n    breaks = c(0, 1, 2, 3, 4, 5),\n    title = \"Number of Busstops\",\n    id = \"grid_id\",\n    showNA = FALSE,\n    alpha = 0.6,\n    popup.vars = c(\n      \"Number of collisions: \" = \"n_colli\"\n    ),\n    popup.format = list(\n      n_colli = list(format = \"f\", digits = 0)\n    )\n  ) +\n  tm_borders(col = \"grey40\", lwd = 0.7)\n\nmap_busstopcounts\n\n\n\n\n\nA few notable findings were:\n\nIn the North-West, bus stops are scarce around the cemetery in Choa Chu Kang, the nearest bus stops in that area are those along Lim Chu Kang road. Tengah Airbase is also located in that area.\nAt the far East, bus stops are scarce around Changi Airport\n- âgrid_idâ = 22027 is an extreme outlier, we will need to drop it\nTowards the middle, we have Paya Lebar Airbase\nIn the middle, we have the Central Water Catchment\nA standalone bus stop in Sentosa Island\n- âgrid_idâ = 11471 is a potential outlier and should be considered for exclusion\nA few bus stops in Johor are surprisingly in our dataset too and in\n- âgrid_idâ = 7068 is an extreme outlier, we will need to drop it.\n- âgrid_idâ for 8113,8237,8351,8485 are potential outliers as well\nOther than those mentioned above, the positioning of the rest of the bus stops seem to be acceptable and will not skew our dataset too much.\n\n\n\nData Cleaning\nHence, letâs proceed straight to dropping these data that will likely cause problems for our analysis. After deeper consideration, we decided that we should drop grid_ids for 22027, 11471 and 7068\n\n\nShow the code\n# Combine Busstop and Hexagon\nhexagon_sf &lt;- hexagon_sf %&gt;%\n  filter(!grid_id %in% c(22027, 11471, 7068))\n\n\n\n\n\nWe needed to perform aggregation of passenger trips by Hexagon instead of Origin Bus Stop, hence we need to first integrate bus stop data and the hexagon dataset using the st_intersection function from the sf package. The intersection operation retains only the spatial elements (points) that overlap between the original bus stop locations and the hexagonal grid.The resulting busstop_hexagon dataset contains information about which hexagon grid each bus stop is located in.\n\n\nShow the code\n# Combine Busstop and Hexagon\nbusstop_hexagon &lt;- st_intersection(busstop, hexagon_sf) %&gt;%\n  select(BUS_STOP_N, grid_id) %&gt;%\n  st_drop_geometry\n\n\nHowever, it is found that there is still one duplicate found despite the thorough cleaning we did earlier:\n- Some bus stops were found to be in multiple grids, this is illogical and should be dropped from analysis.\n-   Example: BUS_STOP 250559 appeared both in grid 4 and 128\n\n\nShow the code\nduplicate &lt;- busstop_hexagon %&gt;%\n  group_by(BUS_STOP_N) %&gt;%\n  filter(n()&gt;1) %&gt;%\n  ungroup()\nduplicate\n\n\n# A tibble: 2 Ã 2\n  BUS_STOP_N grid_id\n  &lt;chr&gt;        &lt;int&gt;\n1 25059            4\n2 25059          128\n\n\nThus, the following preprocessing steps needs to be done:\n\nfilter out bus stops that have multiple grid_ids\n\nThe output now shows that we have successful dealt with duplicates and erroneous data from the integration .\n\n\nShow the code\nbusstop_hexagon &lt;- busstop_hexagon %&gt;%\n  group_by(BUS_STOP_N) %&gt;%\n  filter(n_distinct(grid_id)==1) %&gt;%\n  ungroup()\n\nduplicate &lt;- busstop_hexagon %&gt;%\n  group_by(BUS_STOP_N) %&gt;%\n  filter(n()&gt;1) %&gt;%\n  ungroup()\nduplicate\n\n\n# A tibble: 0 Ã 2\n# â¹ 2 variables: BUS_STOP_N &lt;chr&gt;, grid_id &lt;int&gt;\n\n\nNext, we sum up the total passenger trips group by each hexagon grid as ORIGIN_GRID for the 4 dataframes seperately to get the resulting tibble dataframes.\n\nweekday_morning_peak_join_list &lt;- left_join(weekday_morning_peak , busstop_hexagon,\n            by = c(\"ORIGIN_PT_CODE\" = \"BUS_STOP_N\")) %&gt;%\n  rename(ORIGIN_BS = ORIGIN_PT_CODE, ORIGIN_GRID = grid_id) %&gt;%\n  group_by(ORIGIN_GRID) %&gt;%\n  summarise(TOT_TRIPS = sum(TRIPS))\n\n\n\nweekday_afternoon_peak_join_list &lt;- left_join(weekday_afternoon_peak , busstop_hexagon,\n            by = c(\"ORIGIN_PT_CODE\" = \"BUS_STOP_N\")) %&gt;%\n  rename(ORIGIN_BS = ORIGIN_PT_CODE, ORIGIN_GRID = grid_id) %&gt;%\n  group_by(ORIGIN_GRID) %&gt;%\n  summarise(TOT_TRIPS = sum(TRIPS))\n\n\n\nweekend_morning_peak_join_list &lt;- left_join(weekend_morning_peak , busstop_hexagon,\n            by = c(\"ORIGIN_PT_CODE\" = \"BUS_STOP_N\")) %&gt;%\n  rename(ORIGIN_BS = ORIGIN_PT_CODE, ORIGIN_GRID = grid_id) %&gt;%\n  group_by(ORIGIN_GRID) %&gt;%\n  summarise(TOT_TRIPS = sum(TRIPS))\n\n\n\nweekend_evening_peak_join_list &lt;- left_join(weekend_evening_peak , busstop_hexagon,\n            by = c(\"ORIGIN_PT_CODE\" = \"BUS_STOP_N\")) %&gt;%\n  rename(ORIGIN_BS = ORIGIN_PT_CODE, ORIGIN_GRID = grid_id) %&gt;%\n  group_by(ORIGIN_GRID) %&gt;%\n  summarise(TOT_TRIPS = sum(TRIPS))\n\nAfter that is done, we have to join back with our sf dataset using grid_id.\nThis code chunk below performs several operations to analyze the total number of trips (TOT_TRIPS) during weekday morning peak hours based on the origin bus stop and its corresponding hexagonal grid instead of its previous bus stop number we are using.\n\n\nShow the code\nweekday_morning_peak_join_geometry &lt;- left_join(hexagon_sf, \n                           weekday_morning_peak_join_list,\n                           by = c(\"grid_id\" = \"ORIGIN_GRID\"))\n\nweekday_afternoon_peak_join_geometry &lt;- left_join(hexagon_sf, \n                           weekday_afternoon_peak_join_list,\n                           by = c(\"grid_id\" = \"ORIGIN_GRID\"))\n\nweekend_morning_peak_join_geometry &lt;- left_join(hexagon_sf, \n                           weekend_morning_peak_join_list,\n                           by = c(\"grid_id\" = \"ORIGIN_GRID\"))\n\nweekend_evening_peak_join_geometry &lt;- left_join(hexagon_sf, \n                           weekend_evening_peak_join_list,\n                           by = c(\"grid_id\" = \"ORIGIN_GRID\"))\n\nwrite_rds(weekday_morning_peak_join_geometry, \"data/rds/weekday_morning_peak_join_geometry.rds\")\nweekday_morning_peak_join_geometry &lt;- read_rds(\"data/rds/weekday_morning_peak_join_geometry.rds\")\n\nwrite_rds(weekday_afternoon_peak_join_geometry, \"data/rds/weekday_afternoon_peak_join_geometry.rds\")\nweekday_afternoon_peak_join_geometry &lt;- read_rds(\"data/rds/weekday_afternoon_peak_join_geometry.rds\")\n\nwrite_rds(weekend_morning_peak_join_geometry, \"data/rds/weekend_morning_peak_join_geometry.rds\")\nweekend_morning_peak &lt;- read_rds(\"data/rds/weekend_morning_peak.rds\")\n\nwrite_rds(weekend_evening_peak_join_geometry, \"data/rds/weekend_evening_peak_join_geometry.rds\")\nweekend_evening_peak_join_geometry &lt;- read_rds(\"data/rds/weekend_evening_peak_join_geometry.rds\")"
  },
  {
    "objectID": "Take-Home_Ex/Take-Home_Ex1/Take-Home_Ex1.html#exploratory-data-analysis-eda",
    "href": "Take-Home_Ex/Take-Home_Ex1/Take-Home_Ex1.html#exploratory-data-analysis-eda",
    "title": "Take Home Exercise 1 - Geospatial Analytics for Public Good",
    "section": "3. Exploratory Data Analysis (EDA)",
    "text": "3. Exploratory Data Analysis (EDA)\n\nDistribution of Total TripsDistribution for Total Trips PER Bus StopDistribution for Total Trips Across 4 PeriodsDistribution for Total Trips PER Bus Stop Across 4 Periods\n\n\nWe discovered that the data has a right-tailed distribution for all time classifications.\n\n\nShow the code\ncombined_data &lt;- rbind(\n  transform(weekday_morning_peak_join_geometry, period = \"Weekday Morning Peak\"),\n  transform(weekday_afternoon_peak_join_geometry, period = \"Weekday Afternoon Peak\"),\n  transform(weekend_morning_peak_join_geometry, period = \"Weekend Morning Peak\"),\n  transform(weekend_evening_peak_join_geometry, period = \"Weekend Evening Peak\")\n)\n\n# Plot combined data\nggplot(data = combined_data, \n       aes(x = as.numeric(`TOT_TRIPS`))) +\n  geom_histogram(bins = 20, \n                 color = \"black\", \n                 fill = \"light blue\") +\n  facet_wrap(~period, scales = \"free_y\") +\n  labs(title = \"Distribution of Passenger Trips during Different Time Periods\",\n       subtitle = \"Histograms show the distribution of total trips for different time periods\",\n       x = \"Total Trips\",\n       y = \"Frequency\")\n\n\n\n\n\n\n\nSimilarly, for trips per bus stopâ¦\n\n\nShow the code\ncombined_density &lt;- combined_data %&gt;%\n  mutate(`trips_per_busstop` = (`TOT_TRIPS` / n_colli))\n\n\n# Plot combined data\nggplot(data = combined_density, \n       aes(x = as.numeric(`trips_per_busstop`))) +\n  geom_histogram(bins = 20, \n                 color = \"black\", \n                 fill = \"light blue\") +\n  facet_wrap(~period, scales = \"free_y\") +\n  labs(title = \"Distribution of Passenger Trips during Different Time Periods\",\n       subtitle = \"Histograms show the distribution of total trips for different time periods\",\n       x = \"Total Trips Per BusStop\",\n       y = \"Frequency\")\n\n\n\n\n\n\n\nA quick examination of the distribution of origin trips across all four periods reveals that the weekday morning peak has the highest number of trip counts, followed by the weekday afternoon peak, the weekend morning peak, and finally, the weekend evening peak.\nThis observation suggests that, during the specified time periods, there is a discernible pattern in the frequency of trips, with a notable concentration of trips during weekday mornings. This information could imply potential trends in commuting behavior or specific usage patterns during different times of the week.\nTransport agencies can allocate resources such as personnel and busses better with this information. Frequency of busses should also be increased for weekday afternoon peak period.\n\n\nShow the code\nggplot(combined_data, aes(x = factor(period), y = TOT_TRIPS, fill = factor(period))) +\n  geom_bar(stat = \"identity\", position = \"dodge\") +\n  labs(title = \"Counts of Total Trips Grouped by Periods\",\n       x = \"Period\",\n       y = \"TOT_TRIPS Count\") +\n  theme_minimal() +\n  theme(axis.text.x = element_text(angle = 45, hjust = 1)) \n\n\n\n\n\n\n\nTrips per bus stop turns out to demonstrate similar patterns as compared to total trips.\nThis actually suggest that the transport authorities have done well in the planning of decision of bus stop locations over the years.\nPerhaps this also mean that it is safe to analyse the choropleths using total trips by itself later on.\n\n\nShow the code\nggplot(combined_density, aes(x = factor(period), y = trips_per_busstop, fill = factor(period))) +\n  geom_bar(stat = \"identity\", position = \"dodge\") +\n  labs(title = \"Counts of Trips Per Bus Stop Grouped by Periods\",\n       x = \"Period\",\n       y = \"Trips Per Bus Stop\") +\n  theme_minimal() +\n  theme(axis.text.x = element_text(angle = 45, hjust = 1))"
  },
  {
    "objectID": "Take-Home_Ex/Take-Home_Ex1/Take-Home_Ex1.html#geovisualisation-and-analysis-1",
    "href": "Take-Home_Ex/Take-Home_Ex1/Take-Home_Ex1.html#geovisualisation-and-analysis-1",
    "title": "Take Home Exercise 1 - Geospatial Analytics for Public Good",
    "section": "4. Geovisualisation and Analysis",
    "text": "4. Geovisualisation and Analysis\n\nImportant Considerations\n\nDue to the use of small hexagon tiles and a heavily right skewed distribution, quantile classification proves to provide little value. We decided that kmeans classification is best due to the ability to create discrete classes. This helps us to compare the 4 periods easily.\nWe previously derived number of Passenger Trips using the bus stops as the origin. This means that that some tiles could have missing data due to the lack of trips originating from there but that not necessary be the case for trips with that as the destination.\nWe previously excluded tiles with no bus stops earlier in hexagon_sf, hence any missing data present here is not due to missing bus stops\n\n\n\nGeneral Observations across all 4 interval classifications\n\nThe bus stops along Lim Chu Kang exhibit minimal to no origin trips on both weekdays and weekends. This can be attributed to the presence of a cemetery in that area, making it more practical for individuals to use private transportation.\nAdditionally, along the eastern coast, there are several bus stops without origin trips on weekdays. However, the situation changes on weekends and holidays, although the overall volume remains low. It is advisable to consider adjusting the bus schedule in that region for weekdays. A similar pattern is observed for the islands in the North-West..\nOrigin Trips for a few bus stops near the customs remain high through weekdays and wekends. Also, one bus stop in Johor is consistently high in origin trips.\nIn central areas, origin trips are not high during weekday mornings but are high during weekday afternoons. This is probably due to residential planning by the URA where residents travel from the other regions to the central and business districts.\n\n\nWeekday Morning PeakWeekday Afternoon PeakWeekend/holiday Morning PeakWeekend/holiday Evening Peak\n\n\n\n\nShow the code\ntmap_mode(\"plot\")\ninferno_palette &lt;- inferno(5)\ntmap_options(check.and.fix = TRUE)\ntm_shape(weekday_morning_peak_join_geometry)+\n  tm_fill(\"TOT_TRIPS\", \n          style = \"kmeans\", \n          palette = viridis(5),\n          title = \"Passenger trips\") +\n  tm_layout(main.title = \"Passenger trips during Weekday morning peak\",\n            main.title.position = \"center\",\n            main.title.size = 1.2,\n            legend.height = 0.45, \n            legend.width = 0.35,\n            frame = TRUE) +\n  tm_borders(alpha = 0.5) +\n  tm_scale_bar() +\n  tm_grid(alpha =0.2)+\n  tmap_style(\"natural\")\n\n\n\n\n\n\n\n\n\nShow the code\ntmap_mode(\"plot\")\ntmap_options(check.and.fix = TRUE)\ntm_shape(weekday_afternoon_peak_join_geometry)+\n  tm_fill(\"TOT_TRIPS\", \n          style = \"kmeans\", \n          palette = viridis(5),\n          title = \"Passenger trips\") +\n  tm_layout(main.title = \"Passenger trips during Weekday afternoon peak\",\n            main.title.position = \"center\",\n            main.title.size = 1.2,\n            legend.height = 0.45, \n            legend.width = 0.35,\n            frame = TRUE) +\n  tm_borders(alpha = 0.5) +\n  tm_scale_bar() +\n  tm_grid(alpha =0.2)+\n  tmap_style(\"natural\")\n\n\n\n\n\n\n\n\n\nShow the code\ntmap_mode(\"plot\")\ntmap_options(check.and.fix = TRUE)\ntm_shape(weekend_morning_peak_join_geometry)+\n  tm_fill(\"TOT_TRIPS\", \n          style = \"kmeans\", \n          palette = viridis(5),\n          title = \"Passenger trips\") +\n  tm_layout(main.title = \"Passenger trips during Weekend/holiday morning peak\",\n            main.title.position = \"center\",\n            main.title.size = 1.2,\n            legend.height = 0.45, \n            legend.width = 0.35,\n            frame = TRUE) +\n  tm_borders(alpha = 0.5) +\n  tm_scale_bar() +\n  tm_grid(alpha =0.2)+\n  tmap_style(\"natural\")\n\n\n\n\n\n\n\n\n\nShow the code\ntmap_mode(\"plot\")\ntmap_options(check.and.fix = TRUE)\ntm_shape(weekend_evening_peak_join_geometry)+\n  tm_fill(\"TOT_TRIPS\", \n          style = \"kmeans\",\n          palette = viridis(5), \n          title = \"Passenger trips\") +\n  tm_layout(main.title = \"Passenger trips during Weekend/holiday evening peak\",\n            main.title.position = \"center\",\n            main.title.size = 1.2,\n            legend.height = 0.45, \n            legend.width = 0.35,\n            frame = TRUE) +\n  tm_borders(alpha = 0.5) +\n  tm_scale_bar() +\n  tm_grid(alpha =0.2) +\n  tmap_style(\"natural\")"
  },
  {
    "objectID": "Take-Home_Ex/Take-Home_Ex1/Take-Home_Ex1.html#local-indicators-of-spatial-association-lisa-analysis-1",
    "href": "Take-Home_Ex/Take-Home_Ex1/Take-Home_Ex1.html#local-indicators-of-spatial-association-lisa-analysis-1",
    "title": "Take Home Exercise 1 - Geospatial Analytics for Public Good",
    "section": "5. Local Indicators of Spatial Association (LISA) Analysis",
    "text": "5. Local Indicators of Spatial Association (LISA) Analysis\nLocal Indicators of Spatial Association (LISA) Analysis:\nLocal Indicators of Spatial Association (LISA) is a statistical technique used in spatial analysis to identify and assess spatial patterns of clustering or dispersion within a geographical dataset. LISA analysis helps to uncover local patterns of spatial autocorrelation, providing insights into whether similar values tend to cluster together or if there are areas with dissimilar values.\nLISA analysis is based on the concept of spatial autocorrelation, which measures the degree to which neighboring locations are similar or dissimilar in terms of a particular variable. (In Our case, it is Origin Passenger Trips)\nTwo key LISA statistics are Moranâs I and the associated p-value, for each spatial unit (hexagon grid in our case) to determine if they are part of a significant cluster, outlier, or exhibit no significant pattern.\nLocal Moranâs I identifies clusters by categorizing each unit as High-High (high value surrounded by high values), Low-Low (low value surrounded by low values), High-Low (high value surrounded by low values), or Low-High (low value surrounded by high values).\n- Decision-making process\nDue to the use of hexagon grids, we had many empty grids and this proves difficult to derive contiguity weights. Hence, we attempted to derive distance weights instead\nFixed distance weight matrix was used in deriving the weights and neighbors.\n\n5.1 Deriving adaptive distance weights\nThe summary statistics report below shows that the maximum nearest neighbour distance is 901.4m. By using a threshold value of 902m will ensure that each area will have at least one neighbour.\n\n\nShow the code\ngeo &lt;- sf::st_geometry(hexagon_sf)\nnb &lt;- st_knn(geo, longlat = TRUE)\ndists &lt;- unlist(st_nb_dists(geo, nb))\nsummary(dists)\n\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n  250.0   250.0   250.0   257.6   250.0   901.4 \n\n\nNow we will go ahead to compute the fixed distance weights by using the code chunk below.\nThe use of .allow_zero = TRUE option is to assign the value of 0 to rows with missing values for TOT_TRIPS as missing values will create problems for our analysis later\n\n\nShow the code\nwm_q_1 &lt;- weekday_morning_peak_join_geometry %&gt;%\n  mutate(TOT_TRIPS = replace_na(TOT_TRIPS, 0), \n         nb = st_dist_band(hexagon,\n                           upper = 902),\n         wt = st_weights(nb,\n                         style = \"W\",\n                         allow_zero = TRUE),\n         .before = 1) \n\n\nwm_q_2 &lt;- weekday_afternoon_peak_join_geometry %&gt;%\n  mutate(TOT_TRIPS = replace_na(TOT_TRIPS, 0),  \n         nb = st_dist_band(hexagon,\n                           upper = 902),\n         wt = st_weights(nb,\n                         style = \"W\",\n                         allow_zero = TRUE),\n         .before = 1) \n\nwm_q_3 &lt;- weekend_morning_peak_join_geometry %&gt;%\n  mutate(TOT_TRIPS = replace_na(TOT_TRIPS, 0),  \n         nb = st_dist_band(hexagon,\n                           upper = 902),\n         wt = st_weights(nb,\n                         style = \"W\",\n                         allow_zero = TRUE),\n         .before = 1) \n\n\nwm_q_4 &lt;- weekend_evening_peak_join_geometry %&gt;%\n  mutate(TOT_TRIPS = replace_na(TOT_TRIPS, 0),  \n         nb = st_dist_band(hexagon,\n                           upper = 902),\n         wt = st_weights(nb,\n                         style = \"W\",\n                         allow_zero = TRUE),\n         .before = 1) \n\n\n\n\n5.2 Computing local Moranâs I\nIn this section, we will compute Local Moranâs I of Total Passenger Trips at county level by using local_moran() of sfdep package.\nThe provided code conducts a Local Moranâs I analysis on four distinct datasets (wm_q_1, wm_q_2, wm_q_3, wm_q_4), each associated with specific time periods or scenarios.\nThe analysis focuses on the spatial autocorrelation of the variable TOT_TRIPS within each dataset, employing the local_moran function with\n\nneighbors (nb) and\nweights (wt) and\n99 simulations.\n\nThe calculated Local Moranâs I statistic assesses whether nearby observations exhibit similar total trip values, revealing spatial patterns and clusters.\nThe use of unnest implies a need to extract detailed information about the spatial relationships between observations and their neighbors after the Local Moranâs I analysis.\n\n\nShow the code\nlisa_1 &lt;- wm_q_1 %&gt;% \n  mutate(local_moran = local_moran(\n    TOT_TRIPS, nb, wt, nsim = 99),\n         .before = 1) %&gt;%\n  unnest(local_moran)\n\n\nlisa_2 &lt;- wm_q_2 %&gt;% \n  mutate(local_moran = local_moran(\n    TOT_TRIPS, nb, wt, nsim = 99),\n         .before = 1) %&gt;%\n  unnest(local_moran)\n\n\nlisa_3 &lt;- wm_q_3 %&gt;% \n  mutate(local_moran = local_moran(\n    TOT_TRIPS, nb, wt, nsim = 99),\n         .before = 1) %&gt;%\n  unnest(local_moran)\n\n\nlisa_4 &lt;- wm_q_4 %&gt;% \n  mutate(local_moran = local_moran(\n    TOT_TRIPS, nb, wt, nsim = 99),\n         .before = 1) %&gt;%\n  unnest(local_moran)\n\n\n\n\n5.3 Visualising local Moranâs I\nIn the following code section, tmap functions are utilized to create a choropleth map based on the values in the ii field, representing the Local Moranâs I values. The chosen tmap_style option is set to albatross to suit the gridâs nature and emphasize clusters, where lighter colors indicate positive values and darker colors indicate negative values.\nItâs important to note that a positive Local Moranâs I value signifies a featureâs membership in a cluster, while a negative value suggests that a feature is an outlier.\nExamining the map, regions shaded in various hues of green indicate their membership in one or more clusters.While there are overlapping areas among the maps generated for the four periods of interest, there are also discrepancies.\nHowever, relying solely on the local Moranâs score is insufficient for depicting spatial clustering, as it doesnât provide information about whether the variableâs value (Total Passenger Trips) being examined is high or low, and whether the test result is statistically significant. We need to proceed with analyzing only the regions with statistically significant values of total passenger trips.\n\n5.3.1 Weekday Morning Peak5.3.2 Weekday Afternoon Peak5.3.3 Weekend Morning Peak5.3.4 Weekend Afternoon Peak\n\n\n\n\nShow the code\ntmap_mode(\"plot\")\ntm_shape(lisa_1) +\n  tm_fill(\"ii\",\n          style = \"kmeans\",\n         palette = viridis(5)) + \n  tm_borders(alpha = 0.5) +\n  tm_layout(main.title = \"local Moran I of Bus Trips in weekday morning\",\n            main.title.size = 0.8) +\n  tmap_style(\"albatross\")\n\n\n\n\n\n\n\n\n\nShow the code\ntmap_mode(\"plot\")\ntm_shape(lisa_2) +\n  tm_fill(\"ii\",\n          style = \"kmeans\",\n         palette = viridis(5)) + \n  tm_borders(alpha = 0.5) +\n  tm_layout(main.title = \"local Moran I of Bus Trips in weekday afternoon\",\n            main.title.size = 0.8) +\n  tmap_style(\"albatross\")\n\n\n\n\n\n\n\n\n\nShow the code\ntmap_mode(\"plot\")\ntm_shape(lisa_3) +\n  tm_fill(\"ii\",\n          style = \"kmeans\",\n         palette = viridis(5)) + \n  tm_borders(alpha = 0.5) +\n  tm_layout(main.title = \"local Moran I of Bus Trips in weekend morning\",\n            main.title.size = 0.8) +\n  tmap_style(\"albatross\")\n\n\n\n\n\n\n\n\n\nShow the code\ntmap_mode(\"plot\")\ntm_shape(lisa_4) +\n  tm_fill(\"ii\",\n          style = \"kmeans\",\n         palette = viridis(5)) + \n  tm_borders(alpha = 0.5) +\n  tm_layout(main.title = \"local Moran I of Bus Trips in weekend afternoon\",\n            main.title.size = 0.8) +\n  tmap_style(\"albatross\")\n\n\n\n\n\n\n\n\n\n\n5.4 Visualising p-value of local Moranâs I\nIn the code chunk below, tmap functions are used to prepare a choropleth map by using value in the p_ii_sim field\nWe will visualize solely the statistically significant local Moranâs I values (p_ii_sim &lt; 0.05) through the subsequent code snippet.\n\n5.4.1 Weekday Morning Peak5.4.2 Weekday Afternoon Peak5.4.3 Weekend Morning Peak5.4.4 Weekend Afternoon Peak\n\n\nComparing local Moranâs I together with its p_ii_sim values, a few observations were revealed:\n\nThere are clusters near customs and throughout most of west area. They are all statistically significant.\nClusters and dispersions found in the South / Central are mostly not statistically significant and should be ignored.\nThere are both many clusters and dispersions found in Bedok, Tampines, Pasir Ris and Changi that are found to be statistically significant. However, not so much for other parts of East.\nClusters and Dispersions found in North east and North are also statistically significant.\n\nThese Clusters found seem to match our expectations as they are within residential zones. However, this is possible also because of bus interchanges and the wider options of buses avaliable near these bus interchanges.\nWhat about the dispersion that we are observing? It is easy to dismiss that. However they tend to happen around those clusters found. This can actually be explained with the fact that are OTHER options other than taking buses such as a little bit of walk orâ¦. Cycling!!! as a form of commuting.\nThis trend may not be immediately apparent to individuals who donât utilize public transportation or to foreigners. However, itâs becoming increasingly common for Singaporeans to use personal transport, such as bicycles, to reach bus interchanges instead of waiting for buses directly at their residences. This choice is driven by the realization that cycling to the bus interchange can be a more time-efficient option. The areas surrounding bus interchanges often face congestion with numerous bus stops and traffic lights, turning what should be a short journey into a lengthy one.\n\n\nShow the code\ntmap_mode(\"plot\")\ntm_shape(lisa_1) +\n  tm_fill(\"p_ii_sim\",\n          palette = c(rev(viridis(5)), \"grey\"),\n          breaks = c(0, 0.001, 0.01, 0.05, 1),\n          labels = c(\"0.001\", \"0.01\", \"0.05\", \"Not sig\")) + \n  tm_borders(alpha = 0.5) +\n  tm_layout(main.title = \"p-value of local Moran I in weekday morning\",\n            main.title.size = 0.8) +\n  tmap_style(\"watercolor\")\n\n\n\n\n\n\n\nThere are few dispersions this time round and more clusters found instead.\nMost of previously residential areas identified are found to not display any statistically significant patterns, other than some parts around tampines and north east which could be hubs designated by the government.\nMany statistically significant clusters are found around west region which is also an industrial region.\nCentral business district areas do not exhibit any significant patterns, it seems most office workers do not take buses home but the MRT instead.\n\n\nShow the code\ntmap_mode(\"plot\")\ntm_shape(lisa_2) +\n  tm_fill(\"p_ii_sim\",\n          palette = c(rev(viridis(5)), \"grey\"),\n          breaks = c(0, 0.001, 0.01, 0.05, 1),\n          labels = c(\"0.001\", \"0.01\", \"0.05\", \"Not sig\")) + \n  tm_borders(alpha = 0.5) +\n  tm_layout(main.title = \"p-value of local Moran I in weekday afternoon\",\n            main.title.size = 0.8) +\n  tmap_style(\"watercolor\")\n\n\n\n\n\n\n\nIt seems there are more dispersions during mornings peak hours even for weekends/holidays.\nThere are less clusters as compared to weekdays. The most significant clusters are those found around Bugis and Lavender.\nClusters found near NUS are statistically significant, it is unsure if it is due to the school itself.\n\n\nShow the code\ntmap_mode(\"plot\")\ntm_shape(lisa_3) +\n  tm_fill(\"p_ii_sim\",\n          palette = c(rev(viridis(5)), \"grey\"),\n          breaks = c(0, 0.001, 0.01, 0.05, 1),\n          labels = c(\"0.001\", \"0.01\", \"0.05\", \"Not sig\")) + \n  tm_borders(alpha = 0.5) +\n  tm_layout(main.title = \"p-value of local Moran I in weekend morning\",\n            main.title.size = 0.8) +\n  tmap_style(\"watercolor\")\n\n\n\n\n\n\n\nThere are very few dispersions this time round, with three in Johor, this is not surprising as any Singapore would understand that most wonât choose that time to travel back to Singapore and this data only record trips between Singapore bus stops and does not include trips not under SBS.\nMore clusters are shown and are quite concentrated as compared to weekdays.\n\n\nShow the code\ntmap_mode(\"plot\")\ntm_shape(lisa_4) +\n  tm_fill(\"p_ii_sim\",\n          palette = c(rev(viridis(5)), \"grey\"),\n          breaks = c(0, 0.001, 0.01, 0.05, 1),\n          labels = c(\"0.001\", \"0.01\", \"0.05\", \"Not sig\")) + \n  tm_borders(alpha = 0.5) +\n  tm_layout(main.title = \"p-value of local Moran I in weekend afternoon\",\n            main.title.size = 0.8) +\n  tmap_style(\"watercolor\")"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex3/In-class_Ex3.html",
    "href": "In-class_Ex/In-class_Ex3/In-class_Ex3.html",
    "title": "In-class Exercise 3",
    "section": "",
    "text": "This in-class introduces an alternative R package to spdep package you used in Hands-on Exercise 6. The package is called sfdep. According to Josiah Parry, the developer of the package, âsfdep builds on the great shoulders of spdep package for spatial dependence. sfdep creates an sf and tidyverse friendly interface to the package as well as introduces new functionality that is not present in spdep. sfdep utilizes list columns extensively to make this interface possible.â"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex3/In-class_Ex3.html#overview",
    "href": "In-class_Ex/In-class_Ex3/In-class_Ex3.html#overview",
    "title": "In-class Exercise 3",
    "section": "",
    "text": "This in-class introduces an alternative R package to spdep package you used in Hands-on Exercise 6. The package is called sfdep. According to Josiah Parry, the developer of the package, âsfdep builds on the great shoulders of spdep package for spatial dependence. sfdep creates an sf and tidyverse friendly interface to the package as well as introduces new functionality that is not present in spdep. sfdep utilizes list columns extensively to make this interface possible.â"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex3/In-class_Ex3.html#getting-started",
    "href": "In-class_Ex/In-class_Ex3/In-class_Ex3.html#getting-started",
    "title": "In-class Exercise 3",
    "section": "Getting started",
    "text": "Getting started\n\npacman::p_load(tmap, sf, sp, DT,\n               performance, reshape2,\n               ggpubr, units, tidyverse)\n\n\nmpsz &lt;- st_read(dsn = \"data/geospatial\",\n                   layer = \"MPSZ-2019\") %&gt;%\n  st_transform(crs = 3414)\n\nReading layer `MPSZ-2019' from data source \n  `C:\\weipengten\\ISSS624\\In-class_Ex\\In-class_Ex3\\data\\geospatial' \n  using driver `ESRI Shapefile'\nSimple feature collection with 332 features and 6 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 103.6057 ymin: 1.158699 xmax: 104.0885 ymax: 1.470775\nGeodetic CRS:  WGS 84\n\nmpsz_sp &lt;- as(mpsz, \"Spatial\")\nmpsz_sp\n\nclass       : SpatialPolygonsDataFrame \nfeatures    : 332 \nextent      : 2667.538, 56396.44, 15748.72, 50256.33  (xmin, xmax, ymin, ymax)\ncrs         : +proj=tmerc +lat_0=1.36666666666667 +lon_0=103.833333333333 +k=1 +x_0=28001.642 +y_0=38744.572 +ellps=WGS84 +towgs84=0,0,0,0,0,0,0 +units=m +no_defs \nvariables   : 6\nnames       : SUBZONE_N, SUBZONE_C, PLN_AREA_N, PLN_AREA_C,       REGION_N, REGION_C \nmin values  : ADMIRALTY,    AMSZ01, ANG MO KIO,         AM, CENTRAL REGION,       CR \nmax values  :    YUNNAN,    YSSZ09,     YISHUN,         YS,    WEST REGION,       WR \n\n\n\ndist &lt;- spDists(mpsz_sp, \n                longlat = FALSE)\nhead(dist, n=c(10, 10))\n\n           [,1]       [,2]      [,3]      [,4]       [,5]      [,6]      [,7]\n [1,]     0.000  3926.0025  3939.108 20252.964  2989.9839  1431.330 19211.836\n [2,]  3926.003     0.0000   305.737 16513.865   951.8314  5254.066 16242.523\n [3,]  3939.108   305.7370     0.000 16412.062  1045.9088  5299.849 16026.146\n [4,] 20252.964 16513.8648 16412.062     0.000 17450.3044 21665.795  7229.017\n [5,]  2989.984   951.8314  1045.909 17450.304     0.0000  4303.232 17020.916\n [6,]  1431.330  5254.0664  5299.849 21665.795  4303.2323     0.000 20617.082\n [7,] 19211.836 16242.5230 16026.146  7229.017 17020.9161 20617.082     0.000\n [8,] 14960.942 12749.4101 12477.871 11284.279 13336.0421 16281.453  5606.082\n [9,]  7515.256  7934.8082  7649.776 18427.503  7801.6163  8403.896 14810.930\n[10,]  6391.342  4975.0021  4669.295 15469.566  5226.8731  7707.091 13111.391\n           [,8]      [,9]     [,10]\n [1,] 14960.942  7515.256  6391.342\n [2,] 12749.410  7934.808  4975.002\n [3,] 12477.871  7649.776  4669.295\n [4,] 11284.279 18427.503 15469.566\n [5,] 13336.042  7801.616  5226.873\n [6,] 16281.453  8403.896  7707.091\n [7,]  5606.082 14810.930 13111.391\n [8,]     0.000  9472.024  8575.490\n [9,]  9472.024     0.000  3780.800\n[10,]  8575.490  3780.800     0.000\n\n\n\n16.5.3 Labelling column and row heanders of a distance matrix\n\nsz_names &lt;- mpsz$SUBZONE_C\ncolnames(dist) &lt;- paste0(sz_names)\nrownames(dist) &lt;- paste0(sz_names)\n\n\ndistPair &lt;- melt(dist) %&gt;%\n  rename(dist = value)\nhead(distPair, 10)\n\n     Var1   Var2      dist\n1  MESZ01 MESZ01     0.000\n2  RVSZ05 MESZ01  3926.003\n3  SRSZ01 MESZ01  3939.108\n4  WISZ01 MESZ01 20252.964\n5  MUSZ02 MESZ01  2989.984\n6  MPSZ05 MESZ01  1431.330\n7  WISZ03 MESZ01 19211.836\n8  WISZ02 MESZ01 14960.942\n9  SISZ02 MESZ01  7515.256\n10 SISZ01 MESZ01  6391.342\n\n\n\n\n16.5.5 Updating intra-zonal distances\nIn this section, we are going to append a constant value to replace the intra-zonal distance of 0.\nFirst, we will select and find out the minimum value of the distance by using summary().\n\ndistPair %&gt;%\n  filter(dist &gt; 0) %&gt;%\n  summary()\n\n      Var1             Var2             dist        \n MESZ01 :   331   MESZ01 :   331   Min.   :  173.8  \n RVSZ05 :   331   RVSZ05 :   331   1st Qu.: 7149.5  \n SRSZ01 :   331   SRSZ01 :   331   Median :11890.0  \n WISZ01 :   331   WISZ01 :   331   Mean   :12229.4  \n MUSZ02 :   331   MUSZ02 :   331   3rd Qu.:16401.7  \n MPSZ05 :   331   MPSZ05 :   331   Max.   :49894.4  \n (Other):107906   (Other):107906                    \n\n\nNext, a constant distance value of 50m is added into intra-zones distance. &gt; 50 is derived from approximately minimum of 173.8 (found out earlier in summary statistics) divided by 2. Note : *Intra-zone\n\ndistPair$dist &lt;- ifelse(distPair$dist == 0,\n                        50, distPair$dist)\ndistPair %&gt;%\n  summary()\n\n      Var1             Var2             dist      \n MESZ01 :   332   MESZ01 :   332   Min.   :   50  \n RVSZ05 :   332   RVSZ05 :   332   1st Qu.: 7097  \n SRSZ01 :   332   SRSZ01 :   332   Median :11864  \n WISZ01 :   332   WISZ01 :   332   Mean   :12193  \n MUSZ02 :   332   MUSZ02 :   332   3rd Qu.:16388  \n MPSZ05 :   332   MPSZ05 :   332   Max.   :49894  \n (Other):108232   (Other):108232                  \n\n\n\ndistPair &lt;- distPair %&gt;%\n  rename(orig = Var1,\n         dest = Var2)\n\nwrite_rds(distPair, \"data/rds/distPair.rds\")"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex3/In-class_Ex3.html#preparing-flow-data",
    "href": "In-class_Ex/In-class_Ex3/In-class_Ex3.html#preparing-flow-data",
    "title": "In-class Exercise 3",
    "section": "16.6 Preparing flow data",
    "text": "16.6 Preparing flow data\n\nod_data &lt;- read_rds(\"data/rds/od_data.rds\")\n\nflow_data &lt;- od_data %&gt;%\n  group_by(ORIGIN_SZ, DESTIN_SZ) %&gt;% \n  summarize(TRIPS = sum(MORNING_PEAK)) \n\nhead(flow_data, 10)\n\n# A tibble: 10 Ã 3\n# Groups:   ORIGIN_SZ [1]\n   ORIGIN_SZ DESTIN_SZ TRIPS\n   &lt;chr&gt;     &lt;chr&gt;     &lt;dbl&gt;\n 1 AMSZ01    AMSZ01     2694\n 2 AMSZ01    AMSZ02    10591\n 3 AMSZ01    AMSZ03    14980\n 4 AMSZ01    AMSZ04     3106\n 5 AMSZ01    AMSZ05     7734\n 6 AMSZ01    AMSZ06     2306\n 7 AMSZ01    AMSZ07     1824\n 8 AMSZ01    AMSZ08     2734\n 9 AMSZ01    AMSZ09     2300\n10 AMSZ01    AMSZ10      164\n\n\n\n16.6.1 Separating intra-flow from passenger volume df\n\nflow_data$FlowNoIntra &lt;- ifelse(\n  flow_data$ORIGIN_SZ == flow_data$DESTIN_SZ, \n  0, flow_data$TRIPS)\nflow_data$offset &lt;- ifelse(\n  flow_data$ORIGIN_SZ == flow_data$DESTIN_SZ, \n  0.000001, 1)\n\n\n\n16.6.2 Combining passenger volume data with distance value\nBefore we can join flow_data and distPair, we need to convert data value type of ORIGIN_SZ and DESTIN_SZ fields of flow_data dataframe into factor data type.\n\nflow_data$ORIGIN_SZ &lt;- as.factor(flow_data$ORIGIN_SZ)\nflow_data$DESTIN_SZ &lt;- as.factor(flow_data$DESTIN_SZ)\n\nNow, left_join() of dplyr will be used to merge flow_data dataframe and distPair dataframe. The output is called flow_data1.\n\nflow_data1 &lt;- flow_data %&gt;%\n  left_join (distPair,\n             by = c(\"ORIGIN_SZ\" = \"orig\",\n                    \"DESTIN_SZ\" = \"dest\"))"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex3/In-class_Ex3.html#preparing-origin-and-destination-attributes",
    "href": "In-class_Ex/In-class_Ex3/In-class_Ex3.html#preparing-origin-and-destination-attributes",
    "title": "In-class Exercise 3",
    "section": "16.7 Preparing Origin and Destination Attributes",
    "text": "16.7 Preparing Origin and Destination Attributes\n\n16.7.1 Importing population data\n\npop &lt;- read_csv(\"data/aspatial/pop.csv\")\n\n###v16.7.2 Geospatial data wrangling\n\npop &lt;- pop %&gt;%\n  left_join(mpsz,\n            by = c(\"PA\" = \"PLN_AREA_N\",\n                   \"SZ\" = \"SUBZONE_N\")) %&gt;%\n  select(1:6) %&gt;%\n  rename(SZ_NAME = SZ,\n         SZ = SUBZONE_C)\n\n\n\n16.7.3 Preparing origin attribute\n\nflow_data1 &lt;- flow_data1 %&gt;%\n  left_join(pop,\n            by = c(ORIGIN_SZ = \"SZ\")) %&gt;%\n  rename(ORIGIN_AGE7_12 = AGE7_12,\n         ORIGIN_AGE13_24 = AGE13_24,\n         ORIGIN_AGE25_64 = AGE25_64) %&gt;%\n  select(-c(PA, SZ_NAME))\n\n\n\n16.7.4 Preparing destination attribute\n\nflow_data1 &lt;- flow_data1 %&gt;%\n  left_join(pop,\n            by = c(DESTIN_SZ = \"SZ\")) %&gt;%\n  rename(DESTIN_AGE7_12 = AGE7_12,\n         DESTIN_AGE13_24 = AGE13_24,\n         DESTIN_AGE25_64 = AGE25_64) %&gt;%\n  select(-c(PA, SZ_NAME))\n\nWe will called the output data file SIM_data. it is in rds data file format.\nwrite_rds(flow_data1, âchap16/data/rds/SIM_dataâ)\n16.8 Calibrating Spatial Interaction Models In this section, you will learn how to calibrate Spatial Interaction Models by using Poisson Regression method.\n16.8.1 Importing the modelling data Firstly, let us import the modelling data by using the code chunk below.\nSIM_data &lt;- read_rds(âchap16/data/rds/SIM_data.rdsâ)\n16.8.2 Visualising the dependent variable Firstly, let us plot the distribution of the dependent variable (i.e.Â TRIPS) by using histogram method by using the code chunk below.\nggplot(data = SIM_data, aes(x = TRIPS)) + geom_histogram()"
  },
  {
    "objectID": "Take-Home_Ex/Take-Home_Ex1/Take-Home_Ex1.html#visualising-lisa-map",
    "href": "Take-Home_Ex/Take-Home_Ex1/Take-Home_Ex1.html#visualising-lisa-map",
    "title": "Take Home Exercise 1 - Geospatial Analytics for Public Good",
    "section": "6 Visualising LISA map",
    "text": "6 Visualising LISA map\nIn this visualisation, LISA categorises each region into one of four groups:\n\nHigh-High indicates grids with high number of origin trips located next to other grids with high number of origin trips\nLow-High indicates grids with low number of origin trips located next to other grids with high number of origin trips\nHigh-Low indicates grids with high number of origin trips located next to other grids with low number of origin trips\nLow-Low indicates grids with low number of origin trips located next to other grids with low number of origin trips\n\n\n6.1 Weekday Morning Peak6.2 Weekday Afternoon Peak6.3 Weekend Morning Peak6.4 Weekend Afternoon Peak\n\n\nIn this visualisation for Weekday Morning Peak, some observations were found:\n\nHigh-High regions were found throughout parts of North-East, North-West, Central and West, except South. These spots also seem to be nearby each other in their respective regions, seemingly signifying hubs.\nLow-High regions are few in existence and happened to be near High-High regions\nHigh-Low regions are almost none.\nLow-Low regions happened to be found near the borders of Singapore mostly. They coincide with non-residential areas like changi airport Tuas and near cemetries.\n\n\n\nShow the code\ntmap_mode(\"plot\")\nlisa_sig &lt;- lisa_1  %&gt;%\n  filter(p_ii_sim &lt; 0.05)\ntmap_mode(\"plot\")\ntm_shape(lisa_1) +\n  tm_polygons() +\n  tm_borders(alpha = 0.5) +\ntm_shape(lisa_sig) +\n  tm_fill(\"median\",\n          palette = c(viridis(5))) + \n  tm_borders(alpha = 0.4)+\n  tmap_style(\"albatross\")\n\n\n\n\n\n\n\nHigh-High and High-Low regions are few here and definitely much less than during Weekday Morning Peak.\nThere are three clusters of High-High regions (Jurong west, Woodlands, Bedok), with some Low-High regions around..\nLow-Low regions are found in Tuas Industrial are and throughout parts of Singapore\nMost Residential areas show no patterns.\n\n\nShow the code\ntmap_mode(\"plot\")\nlisa_sig &lt;- lisa_2  %&gt;%\n  filter(p_ii_sim &lt; 0.05)\ntmap_mode(\"plot\")\ntm_shape(lisa_2) +\n  tm_polygons() +\n  tm_borders(alpha = 0.5) +\ntm_shape(lisa_sig) +\n  tm_fill(\"median\",\n          palette = c(viridis(5))) + \n  tm_borders(alpha = 0.4) +\n  tmap_style(\"albatross\")\n\n\n\n\n\n\n\nMore High-High clusters are found with the previous three High-High clusters remaining strong for (Jurong west, Woodlands, Tampines). In addition to those three, are additional clusters found around Ang Mo Kio, Toa Payoh, Bedok and Bugis.\nLow-Low areas are found near Tuas and Changi Airport again and the stretch along cemeteries in Lim Chu Kang\n\n\nShow the code\ntmap_mode(\"plot\")\nlisa_sig &lt;- lisa_3  %&gt;%\n  filter(p_ii_sim &lt; 0.05)\ntmap_mode(\"plot\")\ntm_shape(lisa_3) +\n  tm_polygons() +\n  tm_borders(alpha = 0.5) +\ntm_shape(lisa_sig) +\n  tm_fill(\"median\",\n          palette = c(viridis(5))) + \n  tm_borders(alpha = 0.4) +\n  tmap_style(\"albatross\")\n\n\n\n\n\n\n\nThe patterns for this section seems almost similar to Weekend Morning Peak, most clusters remain but shows less activity compared to its morning.\n\n\nShow the code\ntmap_mode(\"plot\")\nlisa_sig &lt;- lisa_4  %&gt;%\n  filter(p_ii_sim &lt; 0.05)\ntmap_mode(\"plot\")\ntm_shape(lisa_4) +\n  tm_polygons() +\n  tm_borders(alpha = 0.5) +\ntm_shape(lisa_sig) +\n  tm_fill(\"median\",\n          palette = c(viridis(5))) + \n  tm_borders(alpha = 0.4) +\n  tmap_style(\"albatross\")"
  },
  {
    "objectID": "Take-Home_Ex/Take-Home_Ex2/Take-Home_Ex2.html#objectives",
    "href": "Take-Home_Ex/Take-Home_Ex2/Take-Home_Ex2.html#objectives",
    "title": "Take Home Exercise 2 - Regionalisation of Multivariate Water Point Attributes with Non-spatially Constrained and Spatially Constrained Clustering Methods",
    "section": "Objectives",
    "text": "Objectives\nIn this take-home exercise you are required to regionalize Nigeria by using, but not limited to the following measures:\n\nTotal number of functional water points in LGA\nTotal number of nonfunctional water points in LGA\nPercentage of functional water points\nPercentage of non-functional water points\nPercentage of main water point technology (i.e.Â Hand Pump)\nPercentage of usage capacity (i.e.Â &lt; 1000, &gt;=1000)\nPercentage of rural water points"
  },
  {
    "objectID": "Take-Home_Ex/Take-Home_Ex2/Take-Home_Ex2.html#the-data",
    "href": "Take-Home_Ex/Take-Home_Ex2/Take-Home_Ex2.html#the-data",
    "title": "Take Home Exercise 2 - Regionalisation of Multivariate Water Point Attributes with Non-spatially Constrained and Spatially Constrained Clustering Methods",
    "section": "The Data",
    "text": "The Data\n\nAspatial data\nFor the purpose of this assignment, data from WPdx Global Data Repositories will be used. There are two versions of the data. They are: WPdx-Basic and WPdx+. You are required to use WPdx+ data set.\n\n\nGeospatial data\nNigeria Level-2 Administrative Boundary (also known as Local Government Area) polygon features GIS data will be used in this take-home exercise. The data can be downloaded either from The Humanitarian Data Exchange portal or geoBoundaries."
  },
  {
    "objectID": "Take-Home_Ex/Take-Home_Ex2/Take-Home_Ex2.html#the-task",
    "href": "Take-Home_Ex/Take-Home_Ex2/Take-Home_Ex2.html#the-task",
    "title": "Take Home Exercise 2 - Regionalisation of Multivariate Water Point Attributes with Non-spatially Constrained and Spatially Constrained Clustering Methods",
    "section": "The Task",
    "text": "The Task\nThe specific tasks of this take-home exercise are as follows:\n\nUsing appropriate sf method, import the shapefile into R and save it in a simple feature data frame format. Note that there are three Projected Coordinate Systems of Nigeria, they are: EPSG: 26391, 26392, and 26303. You can use any one of them.\nUsing appropriate tidyr and dplyr methods, derive the proportion of functional and non-functional water point at LGA level (i.e.Â ADM2).\nCombining the geospatial and aspatial data frame into simple feature data frame.\nDelineating water point measures functional regions by using conventional hierarchical clustering.\nDelineating water point measures functional regions by using spatially constrained clustering algorithms.\n\n\nThematic Mapping\n\nPlot to show the water points measures derived by using appropriate statistical graphics and choropleth mapping technique.\n\n\n\nAnalytical Mapping\n\nPlot functional regions delineated by using both non-spatially constrained and spatially constrained clustering algorithms.\n\n\n\nInstalling and loading R packages\nBefore we get started, it is important for us to install the necessary R packages into R and launch these R packages into R environment.\nThe R packages needed for this exercise are as follows:\n\nSpatial data handling\n\nsf and sfdep\n\nAttribute data handling\n\ntidyverse (which includes a few essential packages like ggplot2, dplyr, tidyr, readr, purrr, tibble, stringr) and GGally\n\nChoropleth mapping\n\ntmap\n\nMultivariate data visualisation and analysis\n\ncoorplot, ggpubr, heatmaply, palmerpenguins\n\nCluster analysis\n\ncluster, ClustGeo, factoextra\n\n\nThe code chunks below installs and launches these R packages into R environment.\n\npacman::p_load(sf, sfdep, tmap, tidyverse, tmap, ClustGeo, ggpubr, cluster, factoextra, heatmaply,\n               corrplot, GGally, palmerpenguins)"
  },
  {
    "objectID": "Take-Home_Ex/Take-Home_Ex2/Take-Home_Ex2.html#importing-data",
    "href": "Take-Home_Ex/Take-Home_Ex2/Take-Home_Ex2.html#importing-data",
    "title": "Take Home Exercise 2 - Regionalisation of Multivariate Water Point Attributes with Non-spatially Constrained and Spatially Constrained Clustering Methods",
    "section": "1. Importing Data",
    "text": "1. Importing Data\nWe will import the data as a first step before proceeding with data cleaning, data wrangling and data exploration for the following:\n\nwpdx_ngaa, a csv file with the necessary Water Point data that we are interested in,\nnga, a multipolygon feature layer ESRI shapefile format\n\n\nAspatial DataGeospatial Data\n\n\nwpdx_nga is the data that contains the attributes of water points in Nigera that we are interested in, we can import the data simply by using the read_csv function from tidyverse package and output it as a tibble dataframe.\n\nwpdx_nga &lt;- read_csv(\"data/aspatial/eqje-vguj.csv\")\n\nHowever, it also has spatial attributes. Hence, we performed the following steps:\n\nIn order to derive its spatial features represented in the new_georeferenced_column_, we use the st_as_sfc() function of sf package to derive a new field called Geometry.\nNext, st_sf() will be used to convert the tibble data frame into sf data frame geo-referenced to Projected Coordinate Systems of Nigeria using EPSG: 26391.\nWe save our processed data into .rds data format files using theÂ write_rds()Â ofÂ readrÂ package. The output file is saved inÂ rdsÂ sub-folder. We do this to reduce the loading time and more importantly, we can avoid uploading the large raw files onto GitHub.\n\n\n\nShow the code\nwpdx_nga$Geometry = st_as_sfc(wpdx_nga$new_georeferenced_column_)\nwpdx_nga &lt;- st_sf(wpdx_nga, crs=4326)\nst_geometry(wpdx_nga)\n\n\nGeometry set for 1000 features \nGeometry type: POINT\nDimension:     XY\nBounding box:  xmin: 2.875022 ymin: 4.302938 xmax: 13.90153 ymax: 13.39633\nGeodetic CRS:  WGS 84\nFirst 5 geometries:\n\n\nShow the code\nwrite_rds(wpdx_nga, \"data/rds/wpdx_nga.rds\")\nweekday_morning_peak &lt;- read_rds(\"data/rds/wpdx_nga.rds\")\n\n\nThe Geometry column we derived earlier is confirmed to be of POINT type with XY dimension, without a third dimension.\nCRS is in WGS84, we do not perform transformation to Projected CRS for now as st_intersects() only works properly on Geodetic CRS instead of Projected CRS.\n\n\nnga is a geospatial dataset\n\nnga &lt;- st_read(dsn = \"data/geospatial\",\n               layer = \"geoBoundaries-NGA-ADM2\",\n               crs = 4326) %&gt;%\n  select(shapeName)\n\nReading layer `geoBoundaries-NGA-ADM2' from data source \n  `C:\\weipengten\\ISSS624\\Take-Home_Ex\\Take-Home_Ex2\\data\\geospatial' \n  using driver `ESRI Shapefile'\nSimple feature collection with 774 features and 5 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 2.692613 ymin: 4.270204 xmax: 14.67797 ymax: 13.88571\nGeodetic CRS:  WGS 84\n\nwrite_rds(nga, \"data/rds/nga.rds\")\nnga &lt;- read_rds(\"data/rds/nga.rds\")"
  },
  {
    "objectID": "Take-Home_Ex/Take-Home_Ex2/Take-Home_Ex2.html#exploratory-data-analysis-eda",
    "href": "Take-Home_Ex/Take-Home_Ex2/Take-Home_Ex2.html#exploratory-data-analysis-eda",
    "title": "Take Home Exercise 2 - Regionalisation of Multivariate Water Point Attributes with Non-spatially Constrained and Spatially Constrained Clustering Methods",
    "section": "2. Exploratory Data Analysis (EDA)",
    "text": "2. Exploratory Data Analysis (EDA)\n\n2.1 Water Point Data2.2 GeoBoundaries-ADM2 Data\n\n\nA brief check shows no duplcate rows\n\n\nShow the code\nduplicate &lt;- wpdx_nga %&gt;%\ngroup_by_all() %&gt;%\n  filter(n()&gt;1) %&gt;%\n  ungroup()\nduplicate\n\n\nSimple feature collection with 0 features and 73 fields\nBounding box:  xmin: NA ymin: NA xmax: NA ymax: NA\nGeodetic CRS:  WGS 84\n# A tibble: 0 Ã 74\n# â¹ 74 variables: row_id &lt;dbl&gt;, source &lt;chr&gt;, lat_deg &lt;dbl&gt;, lon_deg &lt;dbl&gt;,\n#   report_date &lt;dttm&gt;, status_id &lt;chr&gt;, water_source_clean &lt;chr&gt;,\n#   water_source_category &lt;chr&gt;, water_tech_clean &lt;chr&gt;,\n#   _water_tech_category &lt;chr&gt;, facility_type &lt;chr&gt;, clean_country_name &lt;chr&gt;,\n#   clean_country_id &lt;chr&gt;, clean_adm1 &lt;chr&gt;, clean_adm2 &lt;chr&gt;,\n#   clean_adm3 &lt;lgl&gt;, clean_adm4 &lt;lgl&gt;, install_year &lt;dbl&gt;, installer &lt;chr&gt;,\n#   rehab_year &lt;lgl&gt;, rehabilitator &lt;lgl&gt;, management_clean &lt;chr&gt;, â¦\n\n\nLetâs check into unqiue clean_adm2 values, we have a count of 205 unique LGA boundaries which seems to contradict what we have found in the dataset derived from geoBoundaries.\nWhile clean_adm2 seems to also provide information on LGA boundaries, it seems largely different from that the dataset which we are advised to derived LGA data from. To play safe, we WILL NOT* rely on clean_adm2** but rather the LGA data from geoBoundaries in nga dataframe\n\n\nShow the code\nunique_values_1 &lt;- unique(wpdx_nga$clean_adm2)\nlength(unique_values_1)\n\n\n[1] 205\n\n\nstatus_clean column seems to contain the information of the status of the water points - whether it is functional or non-functional. Letâs check it out by looking at the possible statuses.\n\n\nShow the code\nunique_values_2 &lt;- unique(wpdx_nga$status_clean)\nunique_values_2\n\n\n[1] \"Non-Functional\"           \"Functional, not in use\"  \n[3] \"Abandoned/Decommissioned\"\n\n\n\n\n\n\nShow the code\nduplicate &lt;- nga %&gt;%\ngroup_by_all() %&gt;%\n  filter(n()&gt;1) %&gt;%\n  ungroup()\nduplicate\n\n\nSimple feature collection with 0 features and 1 field\nBounding box:  xmin: NA ymin: NA xmax: NA ymax: NA\nGeodetic CRS:  WGS 84\n# A tibble: 0 Ã 2\n# â¹ 2 variables: shapeName &lt;chr&gt;, geometry &lt;GEOMETRY [Â°]&gt;\n\n\nLetâs check into unqiue ADM values\n\n\nShow the code\nunique_values_3 &lt;- unique(nga$shapeName)\nlength(unique_values_3)\n\n\n[1] 769"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex1/In-class_Ex1.html",
    "href": "In-class_Ex/In-class_Ex1/In-class_Ex1.html",
    "title": "In-class Exercise 1: My First Date with Geospatial Data Science",
    "section": "",
    "text": "In this in-class exercise, you are required to prepare a choropleth map showing the distribution of passenger trips at planning sub-zone by integrating Passenger Volume by Origin Destination Bus Stops and bus stop data sets downloaded from LTA DataMall and Planning Sub-zone boundary of URA Master Plan 2019 downloaded from data.gov.sg.\n\nThe specific task of this in-class exercise are as follows:\n\nto import Passenger Volume by Origin Destination Bus Stops data set downloaded from LTA DataMall in to RStudio environment,\nto import geospatial data in ESRI shapefile format into sf data frame format,\nto perform data wrangling by using appropriate functions from tidyverse and sf pakcges, and\nto visualise the distribution of passenger trip by using tmap methods and functions."
  },
  {
    "objectID": "In-class_Ex/In-class_Ex1/In-class_Ex1.html#the-task",
    "href": "In-class_Ex/In-class_Ex1/In-class_Ex1.html#the-task",
    "title": "In-class Exercise 1: My First Date with Geospatial Data Science",
    "section": "",
    "text": "In this in-class exercise, you are required to prepare a choropleth map showing the distribution of passenger trips at planning sub-zone by integrating Passenger Volume by Origin Destination Bus Stops and bus stop data sets downloaded from LTA DataMall and Planning Sub-zone boundary of URA Master Plan 2019 downloaded from data.gov.sg.\n\nThe specific task of this in-class exercise are as follows:\n\nto import Passenger Volume by Origin Destination Bus Stops data set downloaded from LTA DataMall in to RStudio environment,\nto import geospatial data in ESRI shapefile format into sf data frame format,\nto perform data wrangling by using appropriate functions from tidyverse and sf pakcges, and\nto visualise the distribution of passenger trip by using tmap methods and functions."
  },
  {
    "objectID": "In-class_Ex/In-class_Ex1/In-class_Ex1.html#getting-started",
    "href": "In-class_Ex/In-class_Ex1/In-class_Ex1.html#getting-started",
    "title": "In-class Exercise 1: My First Date with Geospatial Data Science",
    "section": "Getting Started",
    "text": "Getting Started\nThree R packages will be used in this in-class exercise, they are:\n\ntidyverse for non-spatial data handling,\nsf for geospatial data handling,\ntmap for thematic mapping, and\nknitr for creating html table.\n\n\nThe taskThe solution\n\n\nUsing the steps you learned from Hands-on Exercise 1, load these three R packages into RStudio.\n\n\n\npacman::p_load(tmap, sf, tidyverse, \n               knitr)"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex1/In-class_Ex1.html#importing-the-od-data",
    "href": "In-class_Ex/In-class_Ex1/In-class_Ex1.html#importing-the-od-data",
    "title": "In-class Exercise 1: My First Date with Geospatial Data Science",
    "section": "Importing the OD data",
    "text": "Importing the OD data\nFirstly, we will import the Passenger Volume by Origin Destination Bus Stops data set downloaded from LTA DataMall by using read_csv() of readr package.\n\nThe taskThe solution\n\n\nUsing the steps you learned from Hands-on Exercise 1, import origin_destination_bus_202308.csv downloaded from LTA DataMall into RStudio and save it as a tibble data frame called odbus.\n\n\n\nodbus &lt;- read_csv(\"data/aspatial/origin_destination_bus_202308.csv\")\n\n\n\n\nA quick check of odbus tibble data frame shows that the values in OROGIN_PT_CODE and DESTINATON_PT_CODE are in numeric data type.\n\nglimpse(odbus)\n\nRows: 5,709,512\nColumns: 7\n$ YEAR_MONTH          &lt;chr&gt; \"2023-08\", \"2023-08\", \"2023-08\", \"2023-08\", \"2023-â¦\n$ DAY_TYPE            &lt;chr&gt; \"WEEKDAY\", \"WEEKENDS/HOLIDAY\", \"WEEKENDS/HOLIDAY\",â¦\n$ TIME_PER_HOUR       &lt;dbl&gt; 16, 16, 14, 14, 17, 17, 17, 17, 7, 17, 14, 10, 10,â¦\n$ PT_TYPE             &lt;chr&gt; \"BUS\", \"BUS\", \"BUS\", \"BUS\", \"BUS\", \"BUS\", \"BUS\", \"â¦\n$ ORIGIN_PT_CODE      &lt;chr&gt; \"04168\", \"04168\", \"80119\", \"80119\", \"44069\", \"4406â¦\n$ DESTINATION_PT_CODE &lt;chr&gt; \"10051\", \"10051\", \"90079\", \"90079\", \"17229\", \"1722â¦\n$ TOTAL_TRIPS         &lt;dbl&gt; 7, 2, 3, 10, 5, 4, 3, 22, 3, 3, 7, 1, 3, 1, 3, 1, â¦\n\n\n\nThe taskThe solution\n\n\nUsing appropriate tidyverse functions to convert these data values into factor data type.\n\n\n\nodbus$ORIGIN_PT_CODE &lt;- as.factor(odbus$ORIGIN_PT_CODE)\nodbus$DESTINATION_PT_CODE &lt;- as.factor(odbus$DESTINATION_PT_CODE) \n\n\n\n\nNotice that both of them are in factor data type now.\n\nglimpse(odbus)\n\nRows: 5,709,512\nColumns: 7\n$ YEAR_MONTH          &lt;chr&gt; \"2023-08\", \"2023-08\", \"2023-08\", \"2023-08\", \"2023-â¦\n$ DAY_TYPE            &lt;chr&gt; \"WEEKDAY\", \"WEEKENDS/HOLIDAY\", \"WEEKENDS/HOLIDAY\",â¦\n$ TIME_PER_HOUR       &lt;dbl&gt; 16, 16, 14, 14, 17, 17, 17, 17, 7, 17, 14, 10, 10,â¦\n$ PT_TYPE             &lt;chr&gt; \"BUS\", \"BUS\", \"BUS\", \"BUS\", \"BUS\", \"BUS\", \"BUS\", \"â¦\n$ ORIGIN_PT_CODE      &lt;fct&gt; 04168, 04168, 80119, 80119, 44069, 44069, 20281, 2â¦\n$ DESTINATION_PT_CODE &lt;fct&gt; 10051, 10051, 90079, 90079, 17229, 17229, 20141, 2â¦\n$ TOTAL_TRIPS         &lt;dbl&gt; 7, 2, 3, 10, 5, 4, 3, 22, 3, 3, 7, 1, 3, 1, 3, 1, â¦\n\n\n\nExtracting the study data\n\nThe taskThe solution\n\n\nFor the purpose of this exercise, we will extract commuting flows during the weekday morning peak. Call the output tibble data table as origin7_9.\n\n\n\norigin7_9 &lt;- odbus %&gt;%\n  filter(DAY_TYPE == \"WEEKDAY\") %&gt;%\n  filter(TIME_PER_HOUR &gt;= 7 &\n           TIME_PER_HOUR &lt;= 9) %&gt;%\n  group_by(ORIGIN_PT_CODE) %&gt;%\n  summarise(TRIPS = sum(TOTAL_TRIPS))\n\n\n\n\nIt should look similar to the data table below.\n\nkable(head(origin7_9))\n\n\n\n\nORIGIN_PT_CODE\nTRIPS\n\n\n\n\n01012\n1617\n\n\n01013\n813\n\n\n01019\n1620\n\n\n01029\n2383\n\n\n01039\n2727\n\n\n01059\n1415\n\n\n\n\n\nWe will save the output in rds format for future used.\n\nwrite_rds(origin7_9, \"data/rds/origin7_9.rds\")\n\nThe code chunk below will be used to import the save origin7_9.rds into R environment.\n\norigin7_9 &lt;- read_rds(\"data/rds/origin7_9.rds\")"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex1/In-class_Ex1.html#working-with-geospatial-data",
    "href": "In-class_Ex/In-class_Ex1/In-class_Ex1.html#working-with-geospatial-data",
    "title": "In-class Exercise 1: My First Date with Geospatial Data Science",
    "section": "Working with Geospatial Data",
    "text": "Working with Geospatial Data\nIn this section, you are required to import two shapefile into RStudio, they are:\n\nBusStop: This data provides the location of bus stop as at last quarter of 2022.\nMPSZ-2019: This data provides the sub-zone boundary of URA Master Plan 2019.\n\n\nImporting geospatial data\n\nThe taskThe solution\n\n\nUsing the steps you learned from Hands-on Exercise 1, import BusStop downloaded from LTA DataMall into RStudio and save it as a sf data frame called busstop.\n\n\n\nbusstop &lt;- st_read(dsn = \"data/geospatial\",\n                   layer = \"BusStop\") %&gt;%\n  st_transform(crs = 3414)\n\nReading layer `BusStop' from data source \n  `C:\\weipengten\\ISSS624\\In-class_Ex\\In-class_Ex1\\data\\geospatial' \n  using driver `ESRI Shapefile'\nSimple feature collection with 5161 features and 3 fields\nGeometry type: POINT\nDimension:     XY\nBounding box:  xmin: 3970.122 ymin: 26482.1 xmax: 48284.56 ymax: 52983.82\nProjected CRS: SVY21\n\n\n\nduplicates &lt;- busstop[duplicated(busstop$BUS_STOP_N), ]\n\n# Check if there are any duplicates\nif (nrow(duplicates) &gt; 0) {\n  cat(\"Duplicate values found in the BUS_STOP_N column.\\n\")\n  print(duplicates)\n} else {\n  cat(\"No duplicate values found in the BUS_STOP_N column.\\n\")\n}\n\nDuplicate values found in the BUS_STOP_N column.\nSimple feature collection with 16 features and 3 fields\nGeometry type: POINT\nDimension:     XY\nBounding box:  xmin: 13488.02 ymin: 32604.36 xmax: 44055.75 ymax: 47934\nProjected CRS: SVY21 / Singapore TM\nFirst 10 features:\n     BUS_STOP_N BUS_ROOF_N            LOC_DESC                  geometry\n338       58031        UNK     OPP CANBERRA DR POINT (27111.07 47517.77)\n2035      82221        B01              Blk 3A POINT (35308.74 33335.17)\n2038      97079        B14 OPP ST. JOHN'S CRES  POINT (44055.75 38908.5)\n2092      22501        B02            BLK 662A POINT (13488.02 35537.88)\n2237      62251        B03        BEF BLK 471B POINT (35500.36 39943.34)\n3158      53041        B07    Upp Thomson Road POINT (27956.34 37379.29)\n3261      77329        B03   Pasir Ris Central POINT (40728.15 39438.15)\n3265      96319        NIL     YUSEN LOGISTICS POINT (42187.23 34995.78)\n3303      52059        B09             BLK 219 POINT (30565.45 36133.15)\n3411      43709        B06             BLK 644 POINT (18952.02 36751.83)\n\n\n\nduplicates &lt;- busstop[duplicated(busstop$BUS_STOP_N), ]\n\n# Check if there are any duplicates\nif (nrow(duplicates) &gt; 0) {\n  cat(\"Duplicate values found in the BUS_STOP_N column.\\n\")\n  print(duplicates)\n\n  # Remove duplicates from the original dataframe\n  busstop &lt;- busstop[!duplicated(busstop$BUS_STOP_N), ]\n  \n  cat(\"Duplicates removed from the BUS_STOP_N column.\\n\")\n} else {\n  cat(\"No duplicate values found in the BUS_STOP_N column.\\n\")\n}\n\nDuplicate values found in the BUS_STOP_N column.\nSimple feature collection with 16 features and 3 fields\nGeometry type: POINT\nDimension:     XY\nBounding box:  xmin: 13488.02 ymin: 32604.36 xmax: 44055.75 ymax: 47934\nProjected CRS: SVY21 / Singapore TM\nFirst 10 features:\n     BUS_STOP_N BUS_ROOF_N            LOC_DESC                  geometry\n338       58031        UNK     OPP CANBERRA DR POINT (27111.07 47517.77)\n2035      82221        B01              Blk 3A POINT (35308.74 33335.17)\n2038      97079        B14 OPP ST. JOHN'S CRES  POINT (44055.75 38908.5)\n2092      22501        B02            BLK 662A POINT (13488.02 35537.88)\n2237      62251        B03        BEF BLK 471B POINT (35500.36 39943.34)\n3158      53041        B07    Upp Thomson Road POINT (27956.34 37379.29)\n3261      77329        B03   Pasir Ris Central POINT (40728.15 39438.15)\n3265      96319        NIL     YUSEN LOGISTICS POINT (42187.23 34995.78)\n3303      52059        B09             BLK 219 POINT (30565.45 36133.15)\n3411      43709        B06             BLK 644 POINT (18952.02 36751.83)\nDuplicates removed from the BUS_STOP_N column.\n\n\n\n\n\nThe structure of busstop sf tibble data frame should look as below.\n\nglimpse(busstop)\n\nRows: 5,145\nColumns: 4\n$ BUS_STOP_N &lt;chr&gt; \"22069\", \"32071\", \"44331\", \"96081\", \"11561\", \"66191\", \"2338â¦\n$ BUS_ROOF_N &lt;chr&gt; \"B06\", \"B23\", \"B01\", \"B05\", \"B05\", \"B03\", \"B02A\", \"B02\", \"Bâ¦\n$ LOC_DESC   &lt;chr&gt; \"OPP CEVA LOGISTICS\", \"AFT TRACK 13\", \"BLK 239\", \"GRACE INDâ¦\n$ geometry   &lt;POINT [m]&gt; POINT (13576.31 32883.65), POINT (13228.59 44206.38),â¦\n\n\n\nThe taskThe solution\n\n\nUsing the steps you learned from Hands-on Exercise 1, import MPSZ-2019 downloaded from eLearn into RStudio and save it as a sf data frame called mpsz.\n\n\n\nmpsz &lt;- st_read(dsn = \"data/geospatial\",\n                   layer = \"MPSZ-2019\") %&gt;%\n  st_transform(crs = 3414)\n\nReading layer `MPSZ-2019' from data source \n  `C:\\weipengten\\ISSS624\\In-class_Ex\\In-class_Ex1\\data\\geospatial' \n  using driver `ESRI Shapefile'\nSimple feature collection with 332 features and 6 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 103.6057 ymin: 1.158699 xmax: 104.0885 ymax: 1.470775\nGeodetic CRS:  WGS 84\n\n\n\n\n\n\nduplicates &lt;- mpsz[duplicated(mpsz$SUBZONE_C), ]\n\n# Check if there are any duplicates\nif (nrow(duplicates) &gt; 0) {\n  cat(\"Duplicate values found in the SUBZONE_C column.\\n\")\n  print(duplicates)\n} else {\n  cat(\"No duplicate values found in the SUBZONE_C column.\\n\")\n}\n\nNo duplicate values found in the SUBZONE_C column.\n\n\nThe structure of mpsz sf tibble data frame should look as below.\n\nglimpse(mpsz)\n\nRows: 332\nColumns: 7\n$ SUBZONE_N  &lt;chr&gt; \"MARINA EAST\", \"INSTITUTION HILL\", \"ROBERTSON QUAY\", \"JURONâ¦\n$ SUBZONE_C  &lt;chr&gt; \"MESZ01\", \"RVSZ05\", \"SRSZ01\", \"WISZ01\", \"MUSZ02\", \"MPSZ05\",â¦\n$ PLN_AREA_N &lt;chr&gt; \"MARINA EAST\", \"RIVER VALLEY\", \"SINGAPORE RIVER\", \"WESTERN â¦\n$ PLN_AREA_C &lt;chr&gt; \"ME\", \"RV\", \"SR\", \"WI\", \"MU\", \"MP\", \"WI\", \"WI\", \"SI\", \"SI\",â¦\n$ REGION_N   &lt;chr&gt; \"CENTRAL REGION\", \"CENTRAL REGION\", \"CENTRAL REGION\", \"WESTâ¦\n$ REGION_C   &lt;chr&gt; \"CR\", \"CR\", \"CR\", \"WR\", \"CR\", \"CR\", \"WR\", \"WR\", \"CR\", \"CR\",â¦\n$ geometry   &lt;MULTIPOLYGON [m]&gt; MULTIPOLYGON (((33222.98 29..., MULTIPOLYGON (â¦\n\n\n\n\n\n\n\n\nNote\n\n\n\n\nst_read() function of sf package is used to import the shapefile into R as sf data frame.\nst_transform() function of sf package is used to transform the projection to crs 3414."
  },
  {
    "objectID": "In-class_Ex/In-class_Ex1/In-class_Ex1.html#geospatial-data-wrangling",
    "href": "In-class_Ex/In-class_Ex1/In-class_Ex1.html#geospatial-data-wrangling",
    "title": "In-class Exercise 1: My First Date with Geospatial Data Science",
    "section": "Geospatial data wrangling",
    "text": "Geospatial data wrangling\n\nCombining Busstop and mpsz\nCode chunk below populates the planning subzone code (i.e.Â SUBZONE_C) of mpsz sf data frame into busstop sf data frame.\n\nbusstop_mpsz &lt;- st_intersection(busstop, mpsz) %&gt;%\n  select(BUS_STOP_N, SUBZONE_C) %&gt;%\n  st_drop_geometry()\n\n\n\nShow the code\nduplicate &lt;- busstop_mpsz %&gt;%\ngroup_by_all() %&gt;%\n  filter(n()&gt;1) %&gt;%\n  ungroup()\nduplicate\n\n\n# A tibble: 0 Ã 2\n# â¹ 2 variables: BUS_STOP_N &lt;chr&gt;, SUBZONE_C &lt;chr&gt;\n\n\n\n\n\n\n\n\nNote\n\n\n\n\nst_intersection() is used to perform point and polygon overly and the output will be in point sf object.\nselect() of dplyr package is then use to retain only BUS_STOP_N and SUBZONE_C in the busstop_mpsz sf data frame.\nfive bus stops are excluded in the resultant data frame because they are outside of Singapore bpundary.\n\n\n\nBefore moving to the next step, it is wise to save the output into rds format.\n\nwrite_rds(busstop_mpsz, \"data/rds/busstop_mpsz.csv\")  \n\n\nThe taskThe solution\n\n\nNext, we are going to append the planning subzone code from busstop_mpsz data frame onto odbus7_9 data frame.\n\n\n\norigin_SZ &lt;- left_join(origin7_9 , busstop_mpsz,\n            by = c(\"ORIGIN_PT_CODE\" = \"BUS_STOP_N\")) %&gt;%\n  rename(ORIGIN_BS = ORIGIN_PT_CODE,\n         ORIGIN_SZ = SUBZONE_C) %&gt;%\n  group_by(ORIGIN_SZ) %&gt;%\n  summarise(TOT_TRIPS = sum(TRIPS))\n\n\n\n\nBefore continue, it is a good practice for us to check for duplicating records.\n\nduplicate &lt;- origin_SZ %&gt;%\n  group_by_all() %&gt;%\n  filter(n()&gt;1) %&gt;%\n  ungroup()\n\nIf duplicated records are found, the code chunk below will be used to retain the unique records.\n\norigin_data &lt;- unique(origin_SZ)\n\nIt will be a good practice to confirm if the duplicating records issue has been addressed fully.\n\nThe taskThe solution\n\n\nNext, write a code chunk to update od_data data frame with the planning subzone codes.\n\n\n\norigintrip_SZ &lt;- left_join(mpsz, \n                           origin_SZ,\n                           by = c(\"SUBZONE_C\" = \"ORIGIN_SZ\"))"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex1/In-class_Ex1.html#choropleth-visualisation",
    "href": "In-class_Ex/In-class_Ex1/In-class_Ex1.html#choropleth-visualisation",
    "title": "In-class Exercise 1: My First Date with Geospatial Data Science",
    "section": "Choropleth Visualisation",
    "text": "Choropleth Visualisation\n\nThe taskThe solution\n\n\nUsing the steps you had learned, prepare a choropleth map showing the distribution of passenger trips at planning sub-zone level.\n\n\n\ntm_shape(origintrip_SZ)+\n  tm_fill(\"TOT_TRIPS\", \n          style = \"quantile\", \n          palette = \"Blues\",\n          title = \"Passenger trips\") +\n  tm_layout(main.title = \"Passenger trips generated at planning sub-zone level\",\n            main.title.position = \"center\",\n            main.title.size = 1.2,\n            legend.height = 0.45, \n            legend.width = 0.35,\n            frame = TRUE) +\n  tm_borders(alpha = 0.5) +\n  tm_compass(type=\"8star\", size = 2) +\n  tm_scale_bar() +\n  tm_grid(alpha =0.2) +\n  tm_credits(\"Source: Planning Sub-zone boundary from URA\\n and Passenger trips data from LTA\", \n             position = c(\"left\", \"bottom\"))\n\n\n\n\n\n\n\n\nCreating interactive map\n\ntmap_mode(\"plot\")\ntmap_options(check.and.fix = TRUE)\ntm_shape(origintrip_SZ)+\n  tm_fill(\"TOT_TRIPS\", \n          style = \"quantile\", \n          palette = \"Blues\",\n          title = \"Passenger trips\") +\n  tm_layout(main.title = \"Passenger trips generated at planning sub-zone level\",\n            main.title.position = \"center\",\n            main.title.size = 1.2,\n            legend.height = 0.45, \n            legend.width = 0.35,\n            frame = TRUE) +\n  tm_borders(alpha = 0.5) +\n  tm_compass(type=\"8star\", size = 2) +\n  tm_scale_bar() +\n  tm_grid(alpha =0.2) +\n  tm_credits(\"Source: Planning Sub-zone boundary from URA\\n and Passenger trips data from LTA\", \n             position = c(\"left\", \"bottom\"))\n\n\n\n\n:::"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex2/Hands-on_Ex2_2.html",
    "href": "Hands-on_Ex/Hands-on_Ex2/Hands-on_Ex2_2.html",
    "title": "Hands on 2b -âGlobal Measures of Spatial Autocorrelationâ",
    "section": "",
    "text": "In this hands-on exercise, you will learn how to compute Global and Local Measure of Spatial Autocorrelation (GLSA) by using spdep package. By the end to this hands-on exercise, you will be able to:\n\nimport geospatial data using appropriate function(s) of sf package,\nimport csv file using appropriate function of readr package,\nperform relational join using appropriate join function of dplyr package,\ncompute Global Spatial Autocorrelation (GSA) statistics by using appropriate functions of spdep package,\n\nplot Moran scatterplot,\ncompute and plot spatial correlogram using appropriate function of spdep package.\n\ncompute Local Indicator of Spatial Association (LISA) statistics for detecting clusters and outliers by using appropriate functions spdep package;\ncompute Getis-Ordâs Gi-statistics for detecting hot spot or/and cold spot area by using appropriate functions of spdep package; and\nto visualise the analysis output by using tmap package."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex2/Hands-on_Ex2_2.html#overview",
    "href": "Hands-on_Ex/Hands-on_Ex2/Hands-on_Ex2_2.html#overview",
    "title": "Hands on 2b -âGlobal Measures of Spatial Autocorrelationâ",
    "section": "",
    "text": "In this hands-on exercise, you will learn how to compute Global and Local Measure of Spatial Autocorrelation (GLSA) by using spdep package. By the end to this hands-on exercise, you will be able to:\n\nimport geospatial data using appropriate function(s) of sf package,\nimport csv file using appropriate function of readr package,\nperform relational join using appropriate join function of dplyr package,\ncompute Global Spatial Autocorrelation (GSA) statistics by using appropriate functions of spdep package,\n\nplot Moran scatterplot,\ncompute and plot spatial correlogram using appropriate function of spdep package.\n\ncompute Local Indicator of Spatial Association (LISA) statistics for detecting clusters and outliers by using appropriate functions spdep package;\ncompute Getis-Ordâs Gi-statistics for detecting hot spot or/and cold spot area by using appropriate functions of spdep package; and\nto visualise the analysis output by using tmap package."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex2/Hands-on_Ex2_2.html#getting-started",
    "href": "Hands-on_Ex/Hands-on_Ex2/Hands-on_Ex2_2.html#getting-started",
    "title": "Hands on 2b -âGlobal Measures of Spatial Autocorrelationâ",
    "section": "3.2 Getting Started",
    "text": "3.2 Getting Started\n\n3.2.1 The analytical question\nIn spatial policy, one of the main development objective of the local government and planners is to ensure equal distribution of development in the province. Our task in this study, hence, is to apply appropriate spatial statistical methods to discover if development are even distributed geographically. If the answer is No. Then, our next question will be âis there sign of spatial clustering?â. And, if the answer for this question is yes, then our next question will be âwhere are these clusters?â\nIn this case study, we are interested to examine the spatial pattern of a selected development indicator (i.e.Â GDP per capita) of Hunan Provice, People Republic of China.(https://en.wikipedia.org/wiki/Hunan)\n\n\n3.2.2 The Study Area and Data\nTwo data sets will be used in this hands-on exercise, they are:\n\nHunan province administrative boundary layer at county level. This is a geospatial data set in ESRI shapefile format.\nHunan_2012.csv: This csv file contains selected Hunanâs local development indicators in 2012.\n\n\n\n3.2.3 Setting the Analytical Toolls\nBefore we get started, we need to ensure that spdep, sf, tmap and tidyverse packages of R are currently installed in your R.\n\nsf is use for importing and handling geospatial data in R,\ntidyverse is mainly use for wrangling attribute data in R,\nspdep will be used to compute spatial weights, global and local spatial autocorrelation statistics, and\ntmap will be used to prepare cartographic quality chropleth map.\n\nThe code chunk below is used to perform the following tasks:\n\ncreating a package list containing the necessary R packages,\nchecking if the R packages in the package list have been installed in R,\n\nif they have yet to be installed, RStudio will installed the missing packages,\n\nlaunching the packages into R environment.\n\n\npacman::p_load(sf, spdep, tmap, tidyverse)"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex2/Hands-on_Ex2_2.html#getting-the-data-into-r-environment",
    "href": "Hands-on_Ex/Hands-on_Ex2/Hands-on_Ex2_2.html#getting-the-data-into-r-environment",
    "title": "Hands on 2b -âGlobal Measures of Spatial Autocorrelationâ",
    "section": "3.3 Getting the Data Into R Environment",
    "text": "3.3 Getting the Data Into R Environment\nIn this section, you will learn how to bring a geospatial data and its associated attribute table into R environment. The geospatial data is in ESRI shapefile format and the attribute table is in csv fomat.\n\n3.3.1 Import shapefile into r environment\nThe code chunk below uses st_read() of sf package to import Hunan shapefile into R. The imported shapefile will be simple features Object of sf.\n\nhunan &lt;- st_read(dsn = \"data/geospatial\", layer = \"Hunan\")\n\nReading layer `Hunan' from data source \n  `C:\\weipengten\\ISSS624\\Hands-on_Ex\\Hands-on_Ex2\\data\\geospatial' \n  using driver `ESRI Shapefile'\nSimple feature collection with 88 features and 7 fields\nGeometry type: POLYGON\nDimension:     XY\nBounding box:  xmin: 108.7831 ymin: 24.6342 xmax: 114.2544 ymax: 30.12812\nGeodetic CRS:  WGS 84\n\n\n\n\n3.3.2 Import csv file into r environment\nNext, we will import Hunan_2012.csv into R by using read_csv() of readr package. The output is R data frame class.\n\nhunan2012 &lt;- read_csv(\"data/aspatial/Hunan_2012.csv\")\n\nRows: 88 Columns: 29\nââ Column specification ââââââââââââââââââââââââââââââââââââââââââââââââââââââââ\nDelimiter: \",\"\nchr  (2): County, City\ndbl (27): avg_wage, deposite, FAI, Gov_Rev, Gov_Exp, GDP, GDPPC, GIO, Loan, ...\n\nâ¹ Use `spec()` to retrieve the full column specification for this data.\nâ¹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\n\n\n3.3.3 Performing relational join\nThe code chunk below will be used to update the attribute table of hunanâs SpatialPolygonsDataFrame with the attribute fields of hunan2012 dataframe. This is performed by using left_join() of dplyr package.\n\nhunan &lt;- left_join(hunan,hunan2012) %&gt;%   select(1:4, 7, 15)\n\nJoining with `by = join_by(County)`\n\n\n\n\n3.3.4 Visualising Regional Development Indicator\nNow, we are going to prepare a basemap and a choropleth map showing the distribution of GDPPC 2012 by using qtm() of tmap package.\n\nequal &lt;- tm_shape(hunan) +   \n  tm_fill(\"GDPPC\", n = 5, style = \"equal\") +   \n  tm_borders(alpha = 0.5) +   \n  tm_layout(main.title = \"Equal interval classification\")  \nquantile &lt;- tm_shape(hunan) +   \n  tm_fill(\"GDPPC\", n = 5, style = \"quantile\") +   \n  tm_borders(alpha = 0.5) +   \n  tm_layout(main.title = \"Equal quantile classification\")  \ntmap_arrange(equal, quantile, asp=1, ncol=2)"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex2/Hands-on_Ex2_2.html#global-spatial-autocorrelation",
    "href": "Hands-on_Ex/Hands-on_Ex2/Hands-on_Ex2_2.html#global-spatial-autocorrelation",
    "title": "Hands on 2b -âGlobal Measures of Spatial Autocorrelationâ",
    "section": "3.4 Global Spatial Autocorrelation",
    "text": "3.4 Global Spatial Autocorrelation\nIn this section, you will learn how to compute global spatial autocorrelation statistics and to perform spatial complete randomness test for global spatial autocorrelation.\n\n3.4.1 Computing Contiguity Spatial Weights\nBefore we can compute the global spatial autocorrelation statistics, we need to construct a spatial weights of the study area. The spatial weights is used to define the neighbourhood relationships between the geographical units (i.e.Â county) in the study area.\nIn the code chunk below, poly2nb() of spdep package is used to compute contiguity weight matrices for the study area. This function builds a neighbours list based on regions with contiguous boundaries. If you look at the documentation you will see that you can pass a âqueenâ argument that takes TRUE or FALSE as options. If you do not specify this argument the default is set to TRUE, that is, if you donât specify queen = FALSE this function will return a list of first order neighbours using the Queen criteria.\nMore specifically, the code chunk below is used to compute Queen contiguity weight matrix.\n\nwm_q &lt;- poly2nb(hunan, queen=TRUE) \nsummary(wm_q)\n\nNeighbour list object:\nNumber of regions: 88 \nNumber of nonzero links: 448 \nPercentage nonzero weights: 5.785124 \nAverage number of links: 5.090909 \nLink number distribution:\n\n 1  2  3  4  5  6  7  8  9 11 \n 2  2 12 16 24 14 11  4  2  1 \n2 least connected regions:\n30 65 with 1 link\n1 most connected region:\n85 with 11 links\n\n\nThe summary report above shows that there are 88 area units in Hunan. The most connected area unit has 11 neighbours. There are two area units with only one neighbours.\n\n\n3.4.2 Row-standardised weights matrix\nNext, we need to assign weights to each neighboring polygon. In our case, each neighboring polygon will be assigned equal weight (style=âWâ). This is accomplished by assigning the fraction 1/(#ofneighbors) to each neighboring county then summing the weighted income values. While this is the most intuitive way to summaries the neighborsâ values it has one drawback in that polygons along the edges of the study area will base their lagged values on fewer polygons thus potentially over- or under-estimating the true nature of the spatial autocorrelation in the data. For this example, weâll stick with the style=âWâ option for simplicityâs sake but note that other more robust options are available, notably style=âBâ.\n\nrswm_q &lt;- nb2listw(wm_q, style=\"W\", zero.policy = TRUE) \nrswm_q\n\nCharacteristics of weights list object:\nNeighbour list object:\nNumber of regions: 88 \nNumber of nonzero links: 448 \nPercentage nonzero weights: 5.785124 \nAverage number of links: 5.090909 \n\nWeights style: W \nWeights constants summary:\n   n   nn S0       S1       S2\nW 88 7744 88 37.86334 365.9147\n\n\nThe input of nb2listw() must be an object of class nb. The syntax of the function has two major arguments, namely style and zero.poly.\n\nstyle can take values âWâ, âBâ, âCâ, âUâ, âminmaxâ and âSâ. B is the basic binary coding, W is row standardised (sums over all links to n), C is globally standardised (sums over all links to n), U is equal to C divided by the number of neighbours (sums over all links to unity), while S is the variance-stabilizing coding scheme proposed by Tiefelsdorf et al.Â 1999, p.Â 167-168 (sums over all links to n).\nIf zero policy is set to TRUE, weights vectors of zero length are inserted for regions without neighbour in the neighbours list. These will in turn generate lag values of zero, equivalent to the sum of products of the zero row t(rep(0, length=length(neighbours))) %*% x, for arbitrary numerical vector x of length length(neighbours). The spatially lagged value of x for the zero-neighbour region will then be zero, which may (or may not) be a sensible choice.\n\n\n\n3.4.3 Global Spatial Autocorrelation: Moranâs I\nIn this section, you will learn how to perform Moranâs I statistics testing by using moran.test() of spdep.\n\n\n3.4.4 Maronâs I test\nThe code chunk below performs Moranâs I statistical testing using moran.test() of spdep.\n\nmoran.test(hunan$GDPPC,listw=rswm_q, zero.policy = TRUE,na.action=na.omit)\n\n\n    Moran I test under randomisation\n\ndata:  hunan$GDPPC  \nweights: rswm_q    \n\nMoran I statistic standard deviate = 4.7351, p-value = 1.095e-06\nalternative hypothesis: greater\nsample estimates:\nMoran I statistic       Expectation          Variance \n      0.300749970      -0.011494253       0.004348351 \n\n\n\nQuestion: What statistical conclusion can you draw from the output above? In summary there is a statistically significant positive spatial autocorrelation in the GDP per capita variable among the regions, meaning that regions with similar economic characteristics are spatially clustered.\n\n\n3.4.4.1 Computing Monte Carlo Moranâs I\nThe code chunk below performs permutation test for Moranâs I statistic by using moran.mc() of spdep. A total of 1000 simulation will be performed.\n\nset.seed(1234) \nbperm= moran.mc(hunan$GDPPC, listw=rswm_q, nsim=999, zero.policy= TRUE,na.action=na.omit) \nbperm\n\n\n    Monte-Carlo simulation of Moran I\n\ndata:  hunan$GDPPC \nweights: rswm_q  \nnumber of simulations + 1: 1000 \n\nstatistic = 0.30075, observed rank = 1000, p-value = 0.001\nalternative hypothesis: greater\n\n\n\nQuestion: What statistical conclustion can you draw fro mthe output above?\n\n\nstatistic = 0.30075 is more than 0 =&gt; Clustered, observations tend to be similar In summary, the Monte Carlo simulation provides robust evidence that the spatial autocorrelation observed in the GDP per capita variable is unlikely to be due to random chance, supporting the conclusion drawn from the Moranâs I test.\n\n\n\n3.4.4.2 Visualising Monte Carlo Moranâs I\nIt is always a good practice for us the examine the simulated Moranâs I test statistics in greater detail. This can be achieved by plotting the distribution of the statistical values as a histogram by using the code chunk below.\nIn the code chunk below hist() and abline() of R Graphics are used.\n\nmean(bperm$res[1:999])\n\n[1] -0.01504572\n\n\n\nvar(bperm$res[1:999])\n\n[1] 0.004371574\n\n\n\nsummary(bperm$res[1:999])\n\n    Min.  1st Qu.   Median     Mean  3rd Qu.     Max. \n-0.18339 -0.06168 -0.02125 -0.01505  0.02611  0.27593 \n\n\n\nhist(bperm$res,freq=TRUE,  breaks=20, xlab=\"Simulated Moran's I\") \nabline(v=0, col=\"red\") \n\n\n\n\n\nQuestion: What statistical observation can you draw from the output above?\n\n\nThe histogram of simulated Moranâs I values from Monte Carlo simulations represents the distribution of Moranâs I values under the null hypothesis (spatial randomness). The histogram is slightly skewed to the right which confirms the result of Maronâs I test (Moran I statistic of 0.300749970, there is some clustering of similar value)\n\n\nChallenge: Instead of using Base Graph to plot the values, plot the values by using ggplot2 package.\n\n\nlibrary(ggplot2)\n\n# Extract simulated Moran's I values excluding the observed value\nsimulated_values &lt;- bperm$res[1:999]\n\n# Create a ggplot histogram\nggplot(data = data.frame(Moran_I = bperm$res), aes(x = Moran_I)) +\n  geom_histogram(binwidth = 0.02, fill = \"blue\", color = \"black\", alpha = 0.7) +\n  labs(title = \"Histogram of Simulated Moran's I\",\n       x = \"Simulated Moran's I\",\n       y = \"Frequency\") +\n  theme_minimal()\n\n\n\n\n\n\n\n3.4.5 Global Spatial Autocorrelation: Gearyâs\nIn this section, you will learn how to perform Gearyâs c statistics testing by using appropriate functions of spdep package.\n\n3.4.5.1 Gearyâs C test\nThe code chunk below performs Gearyâs C test for spatial autocorrelation by using geary.test() of spdep.\n\ngeary.test(hunan$GDPPC, listw=rswm_q)\n\n\n    Geary C test under randomisation\n\ndata:  hunan$GDPPC \nweights: rswm_q \n\nGeary C statistic standard deviate = 3.6108, p-value = 0.0001526\nalternative hypothesis: Expectation greater than statistic\nsample estimates:\nGeary C statistic       Expectation          Variance \n        0.6907223         1.0000000         0.0073364 \n\n\n\nQuestion: What statistical conclusion can you draw from the output above?\n\n\nThe p-value is very small (0.0001526), indicating that the observed spatial pattern in GDPPC is statistically significant. The Geary C statistic being less than 1 suggests positive spatial autocorrelation, signifying clustering The observed value of Gearyâs C is significantly lower than the expected value under spatial randomness, supporting the conclusion of positive spatial autocorrelation.\n\n\n\n3.4.5.2 Computing Monte Carlo Gearyâs C\nThe code chunk below performs permutation test for Gearyâs C statistic by using geary.mc() of spdep.\n\nset.seed(1234) \nbperm=geary.mc(hunan$GDPPC, listw=rswm_q,nsim=999) \nbperm\n\n\n    Monte-Carlo simulation of Geary C\n\ndata:  hunan$GDPPC \nweights: rswm_q \nnumber of simulations + 1: 1000 \n\nstatistic = 0.69072, observed rank = 1, p-value = 0.001\nalternative hypothesis: greater\n\n\n\nQuestion: What statistical conclusion can you draw from the output above?\n\n\nThe p-value of 0.001 is less than the conventional significance level of 0.05, indicating that the observed Gearyâs C statistic is statistically significant. The alternative hypothesis is that the expectation is greater than the observed statistic, suggesting a positive spatial autocorrelation. The observed Gearyâs C value of 0.69072 is consistent with positive spatial autocorrelation, and the small p-value reinforces the evidence against the null hypothesis of spatial randomness. In summary, the statistical evidence from the Monte Carlo simulation supports the conclusion that there is a significant positive spatial autocorrelation in the distribution of GDPPC values in the study area.\n\n\n\n3.4.5.3 Visualising the Monte Carlo Gearyâs C\nNext, we will plot a histogram to reveal the distribution of the simulated values by using the code chunk below.\n\nmean(bperm$res[1:999])\n\n[1] 1.004402\n\n\n\nvar(bperm$res[1:999])\n\n[1] 0.007436493\n\n\n\nsummary(bperm$res[1:999])\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n 0.7142  0.9502  1.0052  1.0044  1.0595  1.2722 \n\n\n\nhist(bperm$res, freq=TRUE, breaks=20, xlab=\"Simulated Geary c\") \nabline(v=1, col=\"red\") \n\n\n\n\n\nQuestion: What statistical observation can you draw from the output?"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex2/Hands-on_Ex2_2.html#spatial-correlogram",
    "href": "Hands-on_Ex/Hands-on_Ex2/Hands-on_Ex2_2.html#spatial-correlogram",
    "title": "Hands on 2b -âGlobal Measures of Spatial Autocorrelationâ",
    "section": "3.5 Spatial Correlogram",
    "text": "3.5 Spatial Correlogram\nSpatial correlograms are great to examine patterns of spatial autocorrelation in your data or model residuals. They show how correlated are pairs of spatial observations when you increase the distance (lag) between them - they are plots of some index of autocorrelation (Moranâs I or Gearyâs c) against distance.Although correlograms are not as fundamental as variograms (a keystone concept of geostatistics), they are very useful as an exploratory and descriptive tool. For this purpose they actually provide richer information than variograms.\n\n3.5.1 Compute Moranâs I correlogram\nIn the code chunk below, sp.correlogram() of spdep package is used to compute a 6-lag spatial correlogram of GDPPC. The global spatial autocorrelation used in Moranâs I. The plot() of base Graph is then used to plot the output.\n\nMI_corr &lt;- \n  sp.correlogram(wm_q,\n                hunan$GDPPC,                            \n                order=6,                           \n                method=\"I\",                           \n                style=\"W\") \nplot(MI_corr)\n\n\n\n\nBy plotting the output might not allow us to provide complete interpretation. This is because not all autocorrelation values are statistically significant. Hence, it is important for us to examine the full analysis report by printing out the analysis results as in the code chunk below.\n\nprint(MI_corr)\n\nSpatial correlogram for hunan$GDPPC \nmethod: Moran's I\n         estimate expectation   variance standard deviate Pr(I) two sided    \n1 (88)  0.3007500  -0.0114943  0.0043484           4.7351       2.189e-06 ***\n2 (88)  0.2060084  -0.0114943  0.0020962           4.7505       2.029e-06 ***\n3 (88)  0.0668273  -0.0114943  0.0014602           2.0496        0.040400 *  \n4 (88)  0.0299470  -0.0114943  0.0011717           1.2107        0.226015    \n5 (88) -0.1530471  -0.0114943  0.0012440          -4.0134       5.984e-05 ***\n6 (88) -0.1187070  -0.0114943  0.0016791          -2.6164        0.008886 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\nQuestion: What statistical observation can you draw from the plot above?\n\n\nMoranâs I are positive and statistically significant up till the 3rd lag. 5th and 6th lag are negative and statistically significant.\n\n\n\n3.5.2 Compute Gearyâs C correlogram and plot\nIn the code chunk below, sp.correlogram() of spdep package is used to compute a 6-lag spatial correlogram of GDPPC. The global spatial autocorrelation used in Gearyâs C. The plot() of base Graph is then used to plot the output.\n\nGC_corr &lt;- sp.correlogram(wm_q,                            \n                          hunan$GDPPC,                            \n                          order=6,                            \n                          method=\"C\",                            \n                          style=\"W\") \nplot(GC_corr)\n\n\n\n\nSimilar to the previous step, we will print out the analysis report by using the code chunk below.\n\nprint(GC_corr)\n\nSpatial correlogram for hunan$GDPPC \nmethod: Geary's C\n        estimate expectation  variance standard deviate Pr(I) two sided    \n1 (88) 0.6907223   1.0000000 0.0073364          -3.6108       0.0003052 ***\n2 (88) 0.7630197   1.0000000 0.0049126          -3.3811       0.0007220 ***\n3 (88) 0.9397299   1.0000000 0.0049005          -0.8610       0.3892612    \n4 (88) 1.0098462   1.0000000 0.0039631           0.1564       0.8757128    \n5 (88) 1.2008204   1.0000000 0.0035568           3.3673       0.0007592 ***\n6 (88) 1.0773386   1.0000000 0.0058042           1.0151       0.3100407    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1"
  }
]